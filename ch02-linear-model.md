## 线性回归
### 基本概念和术语
线性回归模型是一种特殊类型的监督学习模型。

房价估计，x轴为房子大小，y轴为房价，根据数据集拟合出一条直线，用于根据房子大小预测房价。

术语(Terminology):
1. $x$ = input variable(feature/input feature)
2. $y$ = output(target) variable
3. $m$ = 训练样本数量
4. ($x$, $y$) = 一个训练样本
5. ($x^{(i)}$, $y^{(i)}$) = 第i个训练样本
6. 训练集
7. $f$ = model，通过训练得到的函数，用于预测，输入x，输出$\hat y$
8. $\hat y$ = 预测值，是个估计值，$y$是目标值。
9. $\hat y^{(i)}=f_{w,b}(x^{(i)})=wx^{(i)}+b$，一元线性回归

### 代价函数
代价函数用于找的w和b，使得对于训练集的所有($x^{(i)}$, $y^{(i)}$)都让$\hat y^{(i)}$接近$y^{(i)}$。

误差：$(\hat y^{(i)}-y^{(i)})^2$

整个训练集的误差：$\sum_{i=0}^m(\hat y^{(i)}-y^{(i)})^2$

均方误差：$\frac{1}{m}\sum_{i=0}^m(\hat y^{(i)}-y^{(i)})^2$

代价函数：$J(w,b)=\frac{1}{2m}\sum_{i=0}^m(\hat y^{(i)}-y^{(i)})^2$，均方误差再除2是为了让后续计算简单。称为方差代价函数。

**目标**就是要最小化$J(w,b)$。

### 多元线性回归
到了多元，指的就是某个样本具有多个特征，比如之前的房价预测仅有一个房子大小，现在又加了一些特征（房间数量、楼层和房龄等）。

术语（Terminology）：
1. $x_j$表示第几个特征。
2. $n$表示特征的总数。
3. $\vec{x^{(i)}}$表示第i个训练样本，是个向量，向量中每个元素是特征。
4. $x^{(i)}_j$表示第i个训练样本的第j个特征值。

### 特征工程
利用一些经验设计一些新的特征，通常是对原始特征进行变换或者结合操作。增加的这些新特征，可以使预测变得更准确。如图。
![picture 5](assets/images/1683200664738.png)  

### 多项式回归
Polynomial regression，结合特征工程，对特征进行幂次运算，形成多项式，去拟合。

## 梯度下降
梯度：方向导数的最大值，就是某个方向数值变化最大的向量。下降：变小。

具体可以见下面这个例子，来自于吴恩达课程。
![picture 1](assets/images/1682499401521.png)

解释一下。一个人站在山上，要去往最低的山谷。这时，这个人要旋转360度，找到一个距离他站的位置**坡度最陡**的一个目标点，然后迈出**一步**。重复上述过程，知道走到山谷，当然，这个山谷可能是一个局部最低点。

### 梯度下降算法
需要重复下图两个更新w和b的步骤直到w和b收敛。
![picture 3](assets/images/1682500492211.png)  

**一步**的**长度**就是算法中的学习率，用$α$表示。一般来说$α$是一个0到1之间的数，取0.01, 0.001等。

### 学习率
如果学习率很小的话，那么我们的步子就会很小，这就使代价函数$J(w,b)$的两个参数减小的非常慢，进而导致了最小化代价函数这个过程所用的时间变得很长。

如果学习率很大的话，那么有一种可能会跨过最低点，如图粉色的点。
![picture 4](assets/images/1682501413339.png)  

选择合适的学习率。就是不断尝试，从一个很小的数，比如0.001，然后看loss是否不断下降，如果下降过慢，再逐渐将学习率增大。

### 线性回归的梯度下降
由于均方误差损失函数是个凸函数，所以只有一个全局最小值。
![picture 8](assets/images/1682506956296.png)  

### 逻辑回归的梯度下降
与线性回归类似。如下图。
![picture 14](assets/images/1683356557063.png)  

### 特征放缩
当有不同的特征，并且不同特征的取值范围相差比较大时，可能会导致梯度下降运行缓慢，可以适当对这些特征进行缩放，可以有效加快梯度下降的速度。这是一个小技巧。

#### 如何做
1. 将每个特征都除以自己的最大值，如图。
![picture 1](assets/images/1683198577802.png)  
2. mean normalization，每个特征减去自己的均值，再除以最大值和最小值的差，如图。
![picture 2](assets/images/1683198771985.png)  
3. Z-score normalization，每个特征减去自己的均值，再除以标准差，如图。
![picture 3](assets/images/1683198972086.png)  

### 检查梯度下降是否收敛
两种方法，一个是根据loss的值和迭代次数画出代价下降曲线，另一个是“自动收敛检测”，如下图。吴恩达喜欢第一种方法。
![picture 4](assets/images/1683199984535.png)  

## 逻辑回归
用于解决二分类（0或1，true或false）问题，在此，二分类不是代表好坏，仅代表有两个类别。虽然有回归这个关键词，但是实际是分类问题。

为了实现二分类，常用**sigmoid function**，也叫**logistic function**，可以将输出值控制在0到1之间，选择0.5为阈值，大于0.5的归类为1，小于0.5的归类到0。那等于0.5是什么呢？就是**决策边界**（见下文）。

逻辑回归算法，首先，利用线性回归计算$z=\vec w*\vec x+b$，然后，使用逻辑回归$g(z)=\frac{1}{1+e^{-z}}$得到预测值。如下图。
![picture 6](assets/images/1683277133296.png)  

### 决策边界
假设有两个特征$x_1$和$x_2$，那么线性回归应是$z=\vec w*\vec x+b=w_1x_1+w_2x_2+b$。当上述$z=0$时，也就是$w_1x_1+w_2x_2+b=0$，假设$w_1=1, w_2=1, b=-3$，有$x_1+x_2-3=0$，这一条直线就是**决策边界**。如下图所示，正采样与负采样被决策边界分隔。
![picture 7](assets/images/1683279435518.png)  

上述决策边界是一条直线，即线性的。当然，还有**非线性决策边界**，如下图。
![picture 8](assets/images/1683279599387.png)  

### 代价函数
如果逻辑回归也使用均方误差代价函数，会有多个局部最小值，使用随机梯度下降时，将不会到达全局最小值。如下图，其中函数$L$表示**损失函数**。
![picture 9](assets/images/1683354159007.png)  

逻辑回归的损失函数应该如下两张图所示，充分利用了对数函数的性质。

分类为$1$。当预测值(函数$f$的结果)趋近于$1$时，损失函数趋近$0$，表示$\vec w$和$b$可以；当预测值趋近于$0$时，损失函数趋近于$\infty$。
![picture 10](assets/images/1683354635274.png)  

分类为$0$。当预测值趋近于$0$时，损失函数趋近于$0$，表示$\vec w$和$b$可以；当预测值趋近于$1$时，损失函数趋近于$\infty$。
![picture 11](assets/images/1683354892474.png)  

经过上述探讨，这两个损失函数可以找到全局最小值，也就可以找到某个$\vec w$和$b$使得**代价函数**最小。代价函数如下图。
![picture 12](assets/images/1683355132834.png)  

**简化损失函数**，上述损失函数是一个**分段函数**。但是，由于预测结果$y$非$0$即$1$，所以简化版的损失函数可以写成以下形式（如图）。同时，图中也简化了代价函数，事实上，这个代价函数是由极大似然估计推导出来的（NCF论文中有体现）。
![picture 13](assets/images/1683355828127.png)  

## 过拟合问题
无论是**回归**还是**分类**，如果模型选择的不恰当，都会出现**欠拟合**或者**过拟合**的问题。欠拟合，也可以称为**高偏差**；过拟合，也可以成为**高方差**。

通过回归和分类的两个例子，说明什么是欠拟合，什么是过拟合，什么叫**刚刚好**🤣。如下图。
![picture 15](assets/images/1683357892896.png)  
![picture 16](assets/images/1683357909331.png)  

### 处理过拟合问题
处理过拟合问题有三种方法：增加数据集，特征选择（后续课程介绍）和正则化。

**正则化**，通常的实现方式是**惩罚**所有的$w_j$参数，将正则化项加在**代价函数**的最后。如下图所示，$λ$是正则化项的参数，它的选取需要保持**拟合数据**和**保持$w_j$较小**二者之间的**平衡**。
![picture 17](assets/images/1683361740596.png)  

### 线性回归的正则化
如下图所示。
![picture 18](assets/images/1683362923456.png)  

### 逻辑回归的正则化
与线性回归相似，只是代价函数的差异。如图。
![picture 19](assets/images/1683363371893.png)  
