## NCF(Neural Collaborative Filtering)
**协同过滤**，基于用户过去与商品的互动（比如评价、点击等）来对商品的偏好进行建模。

对于协同过滤，有很多不同的技术，比如矩阵分解（MF）。

**矩阵分解**，将用户和商品投影到共享的潜在空间中，使用潜在的特征向量来表示用户或物品。所以，用户在一个物品上的交互（评价、点击等）就被建模为它们潜在向量的内积。缺点：这种内积可能不足以捕获用户交互数据的复杂结构。

这篇文章探索使用DNNs来从数据中学习交互函数，相比较MF方法，使用DNNs进行推荐所做的工作将更少。

他们关注的是**隐式反馈**，就是那种能间接反映用户偏好的行为（比如观看视频、购买商品、点击物品等）。因为相比**显示反馈**（评价、评级等），**隐式反馈**可以自动追踪，对于内容提供者来说较容易收集。但是，这种方式因为没有观察到用户的满意度，所以**负面反馈**是稀缺的。

所以他们在这个文章中也探讨了利用DNNs来建模**噪声隐式反馈信号**这个中心主题。

并且他们的主要工作如下：
![picture 1](assets/images/1682000366388.png)

### 2. 准备工作
### 2.1 从隐式数据中学习
假设有$M$个用户，$N$个商品。然后定义用户商品交互矩阵$Y$（吴恩达课程中讲的二进制标签）。
> ![picture 2](assets/images/1682034787699.png)  

这里有个问题：如果$y_{ui}=1$，不能说明用户$u$喜欢商品$i$；同理，如果$y_{ui}=0$，也不能说明不喜欢，可能只是用户不知道。所以，矩阵$Y$只提供了关于用户偏好的嘈杂信号。

隐式反馈的推荐问题表述为估计$Y$中未观察条目的分数问题，这些分数用于对条目进行排序。

$\hat y_{ui} = f(u,i|Θ)$，$\hat y_{ui}$表示交互$y_{ui}$预测的分数，$Θ$表示模型参数，$f$表示模型参数到预测分数的映射函数，

为了估计参数$Θ$，现有的方法通常遵循优化目标函数的机器学习范式。文献中常用的目标函数有两种，分别是**点损失**和**成对损失**。

**点损失**，**点学习**，通常遵循回归框架，最小化$\hat y_{ui}$与目标值$y_{ui}$之间的平方损失。对于未观察条目，一种是将其直接视为负反馈，第二种是从未观察条目中采样负反馈实例。

**成对损失**，**成对学习**，最大化观察到的条目$\hat y_{ui}$与未观察到的条目$\hat y_{ui}$之间的余量。观察条目排名应高于未观察条目。

对于**NCF**，使用神经网络将交互函数$f$参数化，来估计$\hat y_{ui}$。因此自然支持**点学习**和**成对学习**。

### 2.2 MF(Matrix Factorization)矩阵分解
定义了两个潜在向量$\vec{p}_u$和$\vec{q}_i$，分别表示用户$u$和物品$i$。矩阵分解使用$\vec{p}_u$和$\vec{q}_i$的**内积**来估计交互值$y_{ui}$。
> ![picture 3](assets/images/1682043424866.png) 

其中，$K$表示潜在空间的**维度**。

由此可以看出，MF建模了用户和物品潜在因素的双向交互（<u>我的理解是因为内积，二者都影响最终结果</u>），并且假设潜在空间的各个维度之间相互独立（<u>各内积各的，不影响</u>）且权重相同（<u>每一项系数都是1</u>），然后线性组合（<u>每一项相加起来</u>）。所以，MF是潜在因素的线性模型。

使用下图做了举例，说明了内积函数是如何限制MF的表达性的。
> ![picture 4](assets/images/1682045242701.png)  

为了更好地理解这个例子，有两个设置要说明一下：
1. 由于MF将用户和物品映射到相同的潜在空间，所以判断两个用户的相似性也是可以用**内积**，或者另一种等价做法是利用两个用户的潜在向量的**夹角的余弦值**（当然，这里假设两个向量都是**单位向量**）。
2. 不失一般性，我们使用**Jaccard coefficient**作为MF需要恢复的两个用户的**真值相似度**。

> **Jaccard coefficient**
> 设$R_u$为用户$u$已经交互过的物品的集合（<u>就是那些$y_{ui}=1$的</u>），然后两个用户$i$和$j$的**Jaccard 相似性**就可以表示为$s_{ij}=\frac{\mid R_i\mid\bigcap\mid R_j\mid}{\mid R_i\mid\bigcup\mid R_j\mid}$，结果是个小数。

根据Figure 1a，可以得出$s_{23}=0.66>s_{12}=0.5>s_{13}=0.4$，故三个用户对应向量$\vec{p}_1 \vec{p}_2 \vec{p}_3$**必然**为Figure 1b的情况（<u>因为$s_{ij}$是相似性</u>）。

加入第四个用户$u_4$，我们可以得到相似性$s_{41}=0.6>s_{43}=0.4>s_{42}=0.2$，说明$u_4$与$u_1$最相似，但无论如何放置$\vec{p}_4$（在先保证与$\vec{p}_1$夹角余弦值最小的情况下）都是距离$\vec{p}_2$更近，而不是$\vec{p}_3$。

上述就是使用固定简单的内积的局限性，当然解决上述问题可以使用大量的$K$，但是又会不利于模型的泛化。所以下一小节将使用DNNs来解决这个限制。