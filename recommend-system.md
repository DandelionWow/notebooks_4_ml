# è¯»ç›¸å…³è®ºæ–‡
## NCF(Neural Collaborative Filtering)
**ååŒè¿‡æ»¤**ï¼ŒåŸºäºç”¨æˆ·è¿‡å»ä¸å•†å“çš„äº’åŠ¨ï¼ˆæ¯”å¦‚è¯„ä»·ã€ç‚¹å‡»ç­‰ï¼‰æ¥å¯¹å•†å“çš„åå¥½è¿›è¡Œå»ºæ¨¡ã€‚

å¯¹äºååŒè¿‡æ»¤ï¼Œæœ‰å¾ˆå¤šä¸åŒçš„æŠ€æœ¯ï¼Œæ¯”å¦‚çŸ©é˜µåˆ†è§£ï¼ˆMFï¼‰ã€‚

**çŸ©é˜µåˆ†è§£**ï¼Œå°†ç”¨æˆ·å’Œå•†å“æŠ•å½±åˆ°å…±äº«çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œä½¿ç”¨æ½œåœ¨çš„ç‰¹å¾å‘é‡æ¥è¡¨ç¤ºç”¨æˆ·æˆ–ç‰©å“ã€‚æ‰€ä»¥ï¼Œç”¨æˆ·åœ¨ä¸€ä¸ªç‰©å“ä¸Šçš„äº¤äº’ï¼ˆè¯„ä»·ã€ç‚¹å‡»ç­‰ï¼‰å°±è¢«å»ºæ¨¡ä¸ºå®ƒä»¬æ½œåœ¨å‘é‡çš„å†…ç§¯ã€‚ç¼ºç‚¹ï¼šè¿™ç§å†…ç§¯å¯èƒ½ä¸è¶³ä»¥æ•è·ç”¨æˆ·äº¤äº’æ•°æ®çš„å¤æ‚ç»“æ„ã€‚

è¿™ç¯‡æ–‡ç« æ¢ç´¢ä½¿ç”¨DNNsæ¥ä»æ•°æ®ä¸­å­¦ä¹ äº¤äº’å‡½æ•°ï¼Œç›¸æ¯”è¾ƒMFæ–¹æ³•ï¼Œä½¿ç”¨DNNsè¿›è¡Œæ¨èæ‰€åšçš„å·¥ä½œå°†æ›´å°‘ã€‚

ä»–ä»¬å…³æ³¨çš„æ˜¯**éšå¼åé¦ˆ**ï¼Œå°±æ˜¯é‚£ç§èƒ½é—´æ¥åæ˜ ç”¨æˆ·åå¥½çš„è¡Œä¸ºï¼ˆæ¯”å¦‚è§‚çœ‹è§†é¢‘ã€è´­ä¹°å•†å“ã€ç‚¹å‡»ç‰©å“ç­‰ï¼‰ã€‚å› ä¸ºç›¸æ¯”**æ˜¾ç¤ºåé¦ˆ**ï¼ˆè¯„ä»·ã€è¯„çº§ç­‰ï¼‰ï¼Œ**éšå¼åé¦ˆ**å¯ä»¥è‡ªåŠ¨è¿½è¸ªï¼Œå¯¹äºå†…å®¹æä¾›è€…æ¥è¯´è¾ƒå®¹æ˜“æ”¶é›†ã€‚ä½†æ˜¯ï¼Œè¿™ç§æ–¹å¼å› ä¸ºæ²¡æœ‰è§‚å¯Ÿåˆ°ç”¨æˆ·çš„æ»¡æ„åº¦ï¼Œæ‰€ä»¥**è´Ÿé¢åé¦ˆ**æ˜¯ç¨€ç¼ºçš„ã€‚

æ‰€ä»¥ä»–ä»¬åœ¨è¿™ä¸ªæ–‡ç« ä¸­ä¹Ÿæ¢è®¨äº†åˆ©ç”¨DNNsæ¥å»ºæ¨¡**å™ªå£°éšå¼åé¦ˆä¿¡å·**è¿™ä¸ªä¸­å¿ƒä¸»é¢˜ã€‚

å¹¶ä¸”ä»–ä»¬çš„ä¸»è¦å·¥ä½œå¦‚ä¸‹ï¼š
![picture 1](assets/images/1682000366388.png)

### 2. å‡†å¤‡å·¥ä½œ
#### 2.1 ä»éšå¼æ•°æ®ä¸­å­¦ä¹ 
å‡è®¾æœ‰$M$ä¸ªç”¨æˆ·ï¼Œ$N$ä¸ªå•†å“ã€‚ç„¶åå®šä¹‰ç”¨æˆ·å•†å“äº¤äº’çŸ©é˜µ$Y$ï¼ˆå´æ©è¾¾è¯¾ç¨‹ä¸­è®²çš„äºŒè¿›åˆ¶æ ‡ç­¾ï¼‰ã€‚
> ![picture 2](assets/images/1682034787699.png)  

è¿™é‡Œæœ‰ä¸ªé—®é¢˜ï¼šå¦‚æœ$y_{ui}=1$ï¼Œä¸èƒ½è¯´æ˜ç”¨æˆ·$u$å–œæ¬¢å•†å“$i$ï¼›åŒç†ï¼Œå¦‚æœ$y_{ui}=0$ï¼Œä¹Ÿä¸èƒ½è¯´æ˜ä¸å–œæ¬¢ï¼Œå¯èƒ½åªæ˜¯ç”¨æˆ·ä¸çŸ¥é“ã€‚æ‰€ä»¥ï¼ŒçŸ©é˜µ$Y$åªæä¾›äº†å…³äºç”¨æˆ·åå¥½çš„å˜ˆæ‚ä¿¡å·ã€‚

éšå¼åé¦ˆçš„æ¨èé—®é¢˜è¡¨è¿°ä¸ºä¼°è®¡$Y$ä¸­æœªè§‚å¯Ÿæ¡ç›®çš„åˆ†æ•°é—®é¢˜ï¼Œè¿™äº›åˆ†æ•°ç”¨äºå¯¹æ¡ç›®è¿›è¡Œæ’åºã€‚

$\hat y_{ui} = f(u,i|Î˜)$ï¼Œ$\hat y_{ui}$è¡¨ç¤ºäº¤äº’$y_{ui}$é¢„æµ‹çš„åˆ†æ•°ï¼Œ$Î˜$è¡¨ç¤ºæ¨¡å‹å‚æ•°ï¼Œ$f$è¡¨ç¤ºæ¨¡å‹å‚æ•°åˆ°é¢„æµ‹åˆ†æ•°çš„æ˜ å°„å‡½æ•°ï¼Œ

ä¸ºäº†ä¼°è®¡å‚æ•°$Î˜$ï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸éµå¾ªä¼˜åŒ–ç›®æ ‡å‡½æ•°çš„æœºå™¨å­¦ä¹ èŒƒå¼ã€‚æ–‡çŒ®ä¸­å¸¸ç”¨çš„ç›®æ ‡å‡½æ•°æœ‰ä¸¤ç§ï¼Œåˆ†åˆ«æ˜¯**ç‚¹æŸå¤±**å’Œ**æˆå¯¹æŸå¤±**ã€‚

**ç‚¹æŸå¤±**ï¼Œ**ç‚¹å­¦ä¹ **ï¼Œé€šå¸¸éµå¾ªå›å½’æ¡†æ¶ï¼Œæœ€å°åŒ–$\hat y_{ui}$ä¸ç›®æ ‡å€¼$y_{ui}$ä¹‹é—´çš„å¹³æ–¹æŸå¤±ã€‚å¯¹äºæœªè§‚å¯Ÿæ¡ç›®ï¼Œä¸€ç§æ˜¯å°†å…¶ç›´æ¥è§†ä¸ºè´Ÿåé¦ˆï¼Œç¬¬äºŒç§æ˜¯ä»æœªè§‚å¯Ÿæ¡ç›®ä¸­é‡‡æ ·è´Ÿåé¦ˆå®ä¾‹ã€‚

**æˆå¯¹æŸå¤±**ï¼Œ**æˆå¯¹å­¦ä¹ **ï¼Œæœ€å¤§åŒ–è§‚å¯Ÿåˆ°çš„æ¡ç›®$\hat y_{ui}$ä¸æœªè§‚å¯Ÿåˆ°çš„æ¡ç›®$\hat y_{ui}$ä¹‹é—´çš„ä½™é‡ã€‚è§‚å¯Ÿæ¡ç›®æ’ååº”é«˜äºæœªè§‚å¯Ÿæ¡ç›®ã€‚

å¯¹äº**NCF**ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œå°†äº¤äº’å‡½æ•°$f$å‚æ•°åŒ–ï¼Œæ¥ä¼°è®¡$\hat y_{ui}$ã€‚å› æ­¤è‡ªç„¶æ”¯æŒ**ç‚¹å­¦ä¹ **å’Œ**æˆå¯¹å­¦ä¹ **ã€‚

#### 2.2 MF(Matrix Factorization)çŸ©é˜µåˆ†è§£
å®šä¹‰äº†ä¸¤ä¸ªæ½œåœ¨å‘é‡$\vec{p}_u$å’Œ$\vec{q}_i$ï¼Œåˆ†åˆ«è¡¨ç¤ºç”¨æˆ·$u$å’Œç‰©å“$i$ã€‚çŸ©é˜µåˆ†è§£ä½¿ç”¨$\vec{p}_u$å’Œ$\vec{q}_i$çš„**å†…ç§¯**æ¥ä¼°è®¡äº¤äº’å€¼$y_{ui}$ã€‚
> ![picture 3](assets/images/1682043424866.png) 

å…¶ä¸­ï¼Œ$K$è¡¨ç¤ºæ½œåœ¨ç©ºé—´çš„**ç»´åº¦**ã€‚

ç”±æ­¤å¯ä»¥çœ‹å‡ºï¼ŒMFå»ºæ¨¡äº†ç”¨æˆ·å’Œç‰©å“æ½œåœ¨å› ç´ çš„åŒå‘äº¤äº’ï¼ˆ<u>æˆ‘çš„ç†è§£æ˜¯å› ä¸ºå†…ç§¯ï¼ŒäºŒè€…éƒ½å½±å“æœ€ç»ˆç»“æœ</u>ï¼‰ï¼Œå¹¶ä¸”å‡è®¾æ½œåœ¨ç©ºé—´çš„å„ä¸ªç»´åº¦ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼ˆ<u>å„å†…ç§¯å„çš„ï¼Œä¸å½±å“</u>ï¼‰ä¸”æƒé‡ç›¸åŒï¼ˆ<u>æ¯ä¸€é¡¹ç³»æ•°éƒ½æ˜¯1</u>ï¼‰ï¼Œç„¶åçº¿æ€§ç»„åˆï¼ˆ<u>æ¯ä¸€é¡¹ç›¸åŠ èµ·æ¥</u>ï¼‰ã€‚æ‰€ä»¥ï¼ŒMFæ˜¯æ½œåœ¨å› ç´ çš„çº¿æ€§æ¨¡å‹ã€‚

ä½¿ç”¨ä¸‹å›¾åšäº†ä¸¾ä¾‹ï¼Œè¯´æ˜äº†å†…ç§¯å‡½æ•°æ˜¯å¦‚ä½•é™åˆ¶MFçš„è¡¨è¾¾æ€§çš„ã€‚
> ![picture 4](assets/images/1682045242701.png)  

ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™ä¸ªä¾‹å­ï¼Œæœ‰ä¸¤ä¸ªè®¾ç½®è¦è¯´æ˜ä¸€ä¸‹ï¼š
1. ç”±äºMFå°†ç”¨æˆ·å’Œç‰©å“æ˜ å°„åˆ°ç›¸åŒçš„æ½œåœ¨ç©ºé—´ï¼Œæ‰€ä»¥åˆ¤æ–­ä¸¤ä¸ªç”¨æˆ·çš„ç›¸ä¼¼æ€§ä¹Ÿæ˜¯å¯ä»¥ç”¨**å†…ç§¯**ï¼Œæˆ–è€…å¦ä¸€ç§ç­‰ä»·åšæ³•æ˜¯åˆ©ç”¨ä¸¤ä¸ªç”¨æˆ·çš„æ½œåœ¨å‘é‡çš„**å¤¹è§’çš„ä½™å¼¦å€¼**ï¼ˆå½“ç„¶ï¼Œè¿™é‡Œå‡è®¾ä¸¤ä¸ªå‘é‡éƒ½æ˜¯**å•ä½å‘é‡**ï¼‰ã€‚
2. ä¸å¤±ä¸€èˆ¬æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨**Jaccard coefficient**ä½œä¸ºMFéœ€è¦æ¢å¤çš„ä¸¤ä¸ªç”¨æˆ·çš„**çœŸå€¼ç›¸ä¼¼åº¦**ã€‚

> **Jaccard coefficient**
> è®¾$R_u$ä¸ºç”¨æˆ·$u$å·²ç»äº¤äº’è¿‡çš„ç‰©å“çš„é›†åˆï¼ˆ<u>å°±æ˜¯é‚£äº›$y_{ui}=1$çš„</u>ï¼‰ï¼Œç„¶åä¸¤ä¸ªç”¨æˆ·$i$å’Œ$j$çš„**Jaccard ç›¸ä¼¼æ€§**å°±å¯ä»¥è¡¨ç¤ºä¸º$s_{ij}=\frac{\mid R_i\mid\bigcap\mid R_j\mid}{\mid R_i\mid\bigcup\mid R_j\mid}$ï¼Œç»“æœæ˜¯ä¸ªå°æ•°ã€‚

æ ¹æ®Figure 1aï¼Œå¯ä»¥å¾—å‡º$s_{23}=0.66>s_{12}=0.5>s_{13}=0.4$ï¼Œæ•…ä¸‰ä¸ªç”¨æˆ·å¯¹åº”å‘é‡$\vec{p}_1 \vec{p}_2 \vec{p}_3$**å¿…ç„¶**ä¸ºFigure 1bçš„æƒ…å†µï¼ˆ<u>å› ä¸º$s_{ij}$æ˜¯ç›¸ä¼¼æ€§</u>ï¼‰ã€‚

åŠ å…¥ç¬¬å››ä¸ªç”¨æˆ·$u_4$ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ç›¸ä¼¼æ€§$s_{41}=0.6>s_{43}=0.4>s_{42}=0.2$ï¼Œè¯´æ˜$u_4$ä¸$u_1$æœ€ç›¸ä¼¼ï¼Œä½†æ— è®ºå¦‚ä½•æ”¾ç½®$\vec{p}_4$ï¼ˆåœ¨å…ˆä¿è¯ä¸$\vec{p}_1$å¤¹è§’ä½™å¼¦å€¼æœ€å°çš„æƒ…å†µä¸‹ï¼‰éƒ½æ˜¯è·ç¦»$\vec{p}_2$æ›´è¿‘ï¼Œè€Œä¸æ˜¯$\vec{p}_3$ã€‚

ä¸Šè¿°å°±æ˜¯ä½¿ç”¨å›ºå®šç®€å•çš„å†…ç§¯çš„å±€é™æ€§ï¼Œå½“ç„¶è§£å†³ä¸Šè¿°é—®é¢˜å¯ä»¥ä½¿ç”¨å¤§é‡çš„$K$ï¼Œä½†æ˜¯åˆä¼šä¸åˆ©äºæ¨¡å‹çš„æ³›åŒ–ã€‚æ‰€ä»¥ä¸‹ä¸€å°èŠ‚å°†ä½¿ç”¨DNNsæ¥è§£å†³è¿™ä¸ªé™åˆ¶ã€‚

### 3. NCF(NEURAL COLLABORATIVE FILTERING)
#### 3.1 æ€»ä½“æ¡†æ¶
ä½¿ç”¨ä¸¤ä¸ªç‰¹å¾å‘é‡è¡¨ç¤ºç”¨æˆ·å’Œç‰©å“ï¼Œä½œä¸ºè¾“å…¥å±‚ï¼Œæ˜¯ç¨€ç–çš„ã€‚ç„¶åä¸Šé¢ä¸€å±‚æ˜¯åµŒå…¥å±‚ï¼Œå°†è¾“å…¥å±‚å‘é‡åŒ–ï¼Œæ˜¯å…¨è¿æ¥å±‚ã€‚å†å¾€ä¸Šæ˜¯ç¥ç»ååŒè¿‡æ»¤å±‚ã€‚æœ€ç»ˆè¾“å‡ºå±‚æ˜¯é¢„æµ‹åˆ†æ•°ï¼Œå¹¶é€šè¿‡æœ€å°åŒ–é€ç‚¹æŸå¤±æ¥è®­ç»ƒã€‚
![picture 5](assets/images/1682067176999.png)  

å°†NCFé¢„æµ‹æ¨¡å‹è¡¨è¿°ä¸ºä¸‹å›¾ï¼š
![picture 6](assets/images/1682079522685.png)  

ç”±äºNCFå±‚æ˜¯å¤šå±‚ï¼Œæ•…åˆè¡¨ç¤ºæˆï¼š
![picture 7](assets/images/1682079569143.png)  

ç”±äºéœ€è¦å°†è¾“å‡ºçš„èŒƒå›´æ§åˆ¶åœ¨$[0,1]$ä¹‹é—´ï¼Œæ‰€ä»¥æ¿€æ´»å‡½æ•°ä½¿ç”¨äº†$sigmoid$ã€‚

ç„¶åï¼Œå®šä¹‰äº†ä¼¼ç„¶å‡½æ•°ï¼š
![picture 8](assets/images/1682079813334.png)  

å¹¶å–ä¼¼ç„¶å‡½æ•°çš„è´Ÿå¯¹æ•°ï¼Œå¾—åˆ°
![picture 9](assets/images/1682079851402.png)  

è¿™å°±æ˜¯NCFè¦æœ€å°åŒ–çš„ç›®æ ‡å‡½æ•°ï¼Œå®ƒçš„ä¼˜åŒ–å¯ä»¥é€šè¿‡éšæœºæ¢¯åº¦ä¸‹é™å®Œæˆã€‚ï¼ˆå› ä¸ºä¸Šæ–‡æåˆ°äº†å¹³æ–¹æŸå¤±ä¸éšå¼æ•°æ®ä¸å¤ªå»åˆï¼Œæ‰€ä»¥å°±ç”¨äº†ä¸Šé¢è¿™ç§äºŒå€¼äº¤å‰ç†µlossï¼‰

#### 3.2 GMF(Generalized Matrix Factorization)
è¯æ˜MFæ˜¯NCFçš„ä¸€ä¸ªç‰¹ä¾‹ã€‚

**ç”¨æˆ·æ½œåœ¨å‘é‡**å°±æ˜¯$P^T\vec v^U_u$ï¼Œ**ç‰©å“æ½œåœ¨å‘é‡**å°±æ˜¯$Q^T\vec v^I_i$ï¼ŒäºŒè€…ä½œä¸ºåµŒå…¥å±‚ã€‚ç„¶åå®šä¹‰NCFå±‚çš„ç¬¬ä¸€å±‚çš„æ˜ å°„å‡½æ•°ä¸º
![picture 10](assets/images/1682083729292.png)  

$\odot$è¡¨ç¤ºä¸¤ä¸ªå‘é‡é€ä¸ªå…ƒç´ ä¹˜ç§¯ã€‚

ç„¶åå°†è¿™ä¸ªç»“æœå‘é‡æŠ•å½±åˆ°è¾“å‡ºå±‚ï¼Œå¾—åˆ°
![picture 11](assets/images/1682083989586.png)  

$a_{out}$è¡¨ç¤ºè¾“å‡ºå±‚æ¿€æ´»å‡½æ•°ï¼Œ$\vec h$è¡¨ç¤ºè¾¹æƒã€‚

<u>ç†è§£ï¼šå°±æ˜¯**ç”¨æˆ·**å’Œ**ç‰©å“**ä¸¤ä¸ªå‘é‡ä½œå…ƒç´ ç›¸ä¹˜ï¼Œç„¶åå†ä¸**è¾¹æƒå‘é‡**ä½œç‚¹ç§¯ï¼Œç„¶åå†åŠ ä¸Š**æ¿€æ´»å‡½æ•°**ï¼Œä½¿ç»“æœç»´æŒåœ¨[0,1]ã€‚</u>

ç¡®å®ï¼Œå¦‚æœ**è¾¹æƒå‘é‡**çš„å…ƒç´ éƒ½ä¸º1ï¼Œ**æ¿€æ´»å‡½æ•°**ä½¿ä¹‹è¾“å‡ºå€¼ä¸å˜ï¼Œçœ‹èµ·æ¥å°±æ˜¯**MF**ã€‚

ç»¼ä¸Šï¼Œå®ç°äº†ä¸€ä¸ªå¹¿ä¹‰çŸ©é˜µåˆ†è§£ï¼Œ**æ¿€æ´»å‡½æ•°**ä¸º$sigmoid$ï¼Œå‘é‡$\vec h$ä½¿ç”¨**äºŒå€¼äº¤å‰ç†µæŸå¤±**ä»æ•°æ®ä¸­å­¦ä¹ ã€‚

#### 3.3 MLP(Multi-Layer Perceptron)
ç”¨æˆ·å’Œç‰©å“å‘é‡è¿›è¡Œæ‹¼æ¥ï¼Œè€Œä¸æ˜¯å¯¹åº”å…ƒç´ ç›¸ä¹˜ã€‚æ¿€æ´»å‡½æ•°ä½¿ç”¨$ReLU$ã€‚ç½‘ç»œç»“æ„ä½¿ç”¨å¡”å¼ï¼Œåº•å±‚æœ€å®½ï¼Œå¾€ä¸Šä¾æ¬¡å‡åŠã€‚

#### 3.4 Fusion of GMF and MLP
åˆ°æ­¤ä¸ºæ­¢å·²ç»ç”¨NCFå¼€å‘äº†ä¸¤ä¸ªå®ä¾‹ï¼Œä¸€ä¸ªæ˜¯è¿ç”¨çº¿æ€§æ ¸å¿ƒå¯¹æ½œåœ¨ç‰¹å¾äº¤äº’å»ºæ¨¡çš„GMFï¼Œå¦ä¸€ä¸ªæ˜¯ä½¿ç”¨éçº¿æ€§ä»æ•°æ®ä¸­å­¦ä¹ äº¤äº’å‡½æ•°çš„MLPã€‚ä¸ºäº†ä½¿äºŒè€…ç›¸äº’å¢å¼ºï¼Œå¹¶ä¸”èƒ½ä¸ºæ›´å¤æ‚çš„ç”¨æˆ·äº¤äº’å»ºæ¨¡ï¼Œæ¥èåˆäºŒè€…ã€‚

ä¸€ç§æœ€ç›´æ¥çš„æ–¹å¼æ˜¯è®©GMFå’ŒMLPå…±äº«ç›¸åŒçš„åµŒå…¥å±‚ï¼Œç„¶åå†ç»„åˆäºŒè€…çš„äº¤äº’å‡½æ•°ä½œä¸ºè¾“å‡ºã€‚å…·ä½“æ¥è¯´ï¼ŒGMFä¸å•å±‚MLPç›¸ç»“åˆçš„æ¨¡å‹å¯ä»¥è¡¨ç¤ºä¸º
![picture 12](assets/images/1682122426637.png)  

ä½†æ˜¯ï¼Œè¿™ç§æ–¹å¼ä¼šé™åˆ¶èåˆå±‚æ¨¡å‹çš„æ€§èƒ½ï¼Œæ¯”å¦‚äºŒè€…éœ€è¦ä½¿ç”¨ç›¸åŒå¤§å°çš„åµŒå…¥å±‚ï¼Œä½†äºŒè€…çš„æœ€ä¼˜åµŒå…¥å±‚å¯èƒ½ä¸ä¸€æ ·ï¼Œæ‰€ä»¥çµæ´»æ€§ä¸å¥½ã€‚

ä¸ºäº†æ›´å…·çµæ´»æ€§ï¼Œè®©äºŒè€…ä½¿ç”¨ä¸åŒçš„åµŒå…¥å±‚ï¼Œè¿æ¥å®ƒä»¬æœ€åä¸€ä¸ªéšè—å±‚æ¥ç»„åˆä¸¤ä¸ªæ¨¡å‹ã€‚å›¾å’Œå…¬å¼å¦‚ä¸‹
![picture 13](assets/images/1682122958707.png)  

![picture 14](assets/images/1682122972816.png)  

å…¶ä¸­ï¼Œ$\vec p^G_u$å’Œ$\vec p^M_u$åˆ†åˆ«è¡¨ç¤ºGMFå’ŒMLPçš„ç”¨æˆ·åµŒå…¥å±‚ï¼ŒåŒç†ï¼Œ$\vec q^G_i$å’Œ$\vec q^M_i$è¡¨ç¤ºç‰©å“çš„ã€‚åŒä¸Šæ–‡ï¼ŒMLPå±‚çš„æ¿€æ´»å‡½æ•°ä½¿ç”¨$ReLU$ã€‚æœ€åï¼Œç§°è¿™ä¸ªæ¨¡å‹ä¸º"NeuMF(Neural Matrix Factorization)"ã€‚æ¨¡å‹ä¸­å…³äºæ¯ä¸ªå‚æ•°çš„å¯¼æ•°éƒ½å¯ä»¥é€šè¿‡åå‘ä¼ æ’­è®¡ç®—ã€‚

##### 3.4.1 é¢„è®­ç»ƒ
æ–‡çŒ®ä¸­æ¨èä½¿ç”¨é¢„è®­ç»ƒï¼Œäº‹å…ˆè®­ç»ƒGMFå’ŒMLPçš„æ¨¡å‹ç›´åˆ°æ”¶æ•›ï¼Œç„¶åå°†äºŒè€…çš„æ¨¡å‹å‚æ•°ä½œä¸ºNeuMFçš„ç›¸åº”éƒ¨åˆ†å‚æ•°çš„åˆå§‹åŒ–ã€‚å”¯ä¸€çš„è°ƒæ•´æ˜¯åœ¨è¾“å‡ºå±‚ï¼Œå°†ä¸¤ä¸ªæ¨¡å‹çš„å‚æ•°ä½œè¿æ¥æ“ä½œï¼š
![picture 15](assets/images/1682128305667.png)  

å…¶ä¸­ï¼Œ$Î±$è¡¨ç¤ºå†³å®šä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡å‹ä¹‹é—´æƒé‡çš„è¶…å‚æ•°ï¼ˆ~~åœ¨ä»£ç ä¸­å¥½åƒåªåšäº†è¿æ¥æ“ä½œï¼Œæ²¡æœ‰åŠ å…¥è¿™ä¸ªè¶…å‚æ•°~~ã€‚ä»£ç ä¸­$Î±$æ˜¯0.5ï¼Œåœ¨ä¸‹æ–‡å®éªŒå°èŠ‚ä¹Ÿæåˆ°äº†ï¼‰ã€‚

å¯¹äºè®­ç»ƒGMFå’ŒMLPï¼Œé‡‡ç”¨äº†è‡ªé€‚åº”çŸ©ä¼°è®¡ï¼Œå®ƒé€šè¿‡å¯¹é¢‘ç¹çš„å‚æ•°æ‰§è¡Œè¾ƒå°çš„æ›´æ–°å’Œå¯¹ä¸é¢‘ç¹çš„å‚æ•°æ‰§è¡Œè¾ƒå¤§çš„æ›´æ–°æ¥é€‚åº”æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ã€‚åœ¨å°†é¢„è®­ç»ƒçš„å‚æ•°è¾“å…¥NeuMFåï¼Œæˆ‘ä»¬ä½¿ç”¨æœ´ç´ SGDè€Œä¸æ˜¯Adamæ¥ä¼˜åŒ–å®ƒã€‚ï¼ˆ<u>ä½†æ˜¯ï¼Œåœ¨ä»£ç ä¸­å¹¶æ²¡æœ‰è§åˆ°ä½¿ç”¨SGDï¼Œé€šè¿‡å¯¹ä»£ç çš„debugï¼Œå‘ç°è®­ç»ƒä¹‹å‰ï¼Œä¼˜åŒ–å™¨è¿˜æ˜¯adam</u>ï¼‰

### 4. EXPERIMENTS
å®éªŒçš„ç›®çš„æ˜¯ä¸ºäº†è§£è¯»ä¸‹é¢ä¸‰ä¸ªé—®é¢˜ï¼š
1. æˆ‘ä»¬æå‡ºçš„NCFæ–¹æ³•æ˜¯å¦ä¼˜äºæœ€å…ˆè¿›çš„éšå¼ååŒè¿‡æ»¤æ–¹æ³•?
2. æˆ‘ä»¬æå‡ºçš„ä¼˜åŒ–æ¡†æ¶(è´Ÿé‡‡æ ·çš„å¯¹æ•°æŸå¤±)å¦‚ä½•ç”¨äºæ¨èä»»åŠ¡?
3. æ›´æ·±å±‚æ¬¡çš„éšè—å•å…ƒæ˜¯å¦æœ‰åŠ©äºä»ç”¨æˆ·é¡¹ç›®äº¤äº’æ•°æ®ä¸­å­¦ä¹ ?

#### 4.1 å®éªŒè®¾ç½®
**æ•°æ®é›†**é€‰æ‹©äº†ä¸¤ä¸ªï¼Œåˆ†åˆ«æ˜¯**MovieLens**å’Œ**Pinterest**ã€‚

**MovieLens**æ˜¯ä¸€ä¸ªæ˜¾ç¤ºåé¦ˆæ•°æ®é›†ï¼Œåœ¨æœ¬é¡¹ç›®ä¸­ï¼Œè½¬ä¸ºäº†éšå¼åé¦ˆæ•°æ®é›†ï¼ˆå³æ¯ä¸ªæ¡ç›®è¢«æ ‡è®°ä¸º0æˆ–1ï¼‰ã€‚

**è¯„ä»·åè®®**ï¼Œæ–‡çŒ®ä¸­çš„è¯„ä»·æ–¹æ³•é‡‡ç”¨ç•™ä¸€æ³•äº¤å‰éªŒè¯ï¼Œæ€§èƒ½æŒ‡æ ‡ä½¿ç”¨HR(Hit Ratio)å’ŒNDCG(Normalized Discounted Cumulative Gain)ã€‚

> <a href='https://zhuanlan.zhihu.com/p/493958358'>HRå’ŒNDCGçš„çŸ¥ä¹æ–‡ç« </a>
> 
> **HR**ï¼Œå¼ºè°ƒçš„æ˜¯æ¨¡å‹æ¨èçš„å‡†ç¡®æ€§ï¼Œå³ç”¨æˆ·çš„éœ€æ±‚é¡¹æ˜¯å¦åŒ…å«åœ¨æ¨¡å‹çš„æ¨èé¡¹ä¸­ã€‚
> **NDCG**ï¼Œå¼ºè°ƒçš„æ˜¯ç”¨æˆ·çš„éœ€æ±‚é¡¹åœ¨æ¨¡å‹æ¨èåˆ—è¡¨ä¸­çš„ä½ç½®ï¼Œè¶Šé å‰è¶Šä½³ã€‚

**Baselines**ï¼Œé€‰å‡º4ä¸ªæ¨¡å‹ä¸NCFçš„å®ä¾‹ï¼ˆGMFã€MLPå’ŒNeuMFï¼‰æ¯”è¾ƒã€‚åˆ†åˆ«æ˜¯ï¼š**ItemPop**ã€**ItemKNN**ã€**BPR**å’Œ**eALS**ã€‚

**å‚æ•°è®¾ç½®**ï¼ŒæŸå¤±å‡½æ•°å‡ä½¿ç”¨**log loss**ï¼Œæ¯ä¸ªæ­£å®ä¾‹å‡å¢åŠ å››ä¸ªè´Ÿå®ä¾‹ï¼Œåˆå§‹åŒ–æ¨¡å‹å‚æ•°ä½¿ç”¨**é«˜æ–¯åˆ†å¸ƒ**ï¼ˆå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º0.01ï¼‰ï¼Œ**batch_size**æµ‹è¯•äº†[128, 256, 512, 1024]ï¼Œ**å­¦ä¹ ç‡**æµ‹è¯•äº†[0.0001, 0.0005, 0.001, 0.005]ï¼Œ**é¢„æµ‹å› å­**æµ‹è¯•äº†[8, 16, 32, 64]ï¼ŒMLPçš„éšå«å±‚æœ‰3å±‚ã€‚

æ–‡çŒ®å‰©ä¸‹çš„å†…å®¹éƒ½åœ¨è®²å›¾äº†å°±ã€‚

### æ€»ç»“
é€šè¿‡è¿™ç¯‡æ–‡ç« ï¼Œæˆ‘äº†è§£äº†æ¨èç³»ç»Ÿå…¶ä¸­çš„ä¸€ä¸ªæ–¹å‘ï¼šååŒè¿‡æ»¤ã€‚åœ¨è¿™ç¯‡æ–‡çŒ®ä¸­ï¼Œä½œè€…å°†ç¥ç»ç½‘ç»œä¸ååŒè¿‡æ»¤ç›¸ç»“åˆï¼Œæå‡ºäº†ç¥ç»ååŒè¿‡æ»¤æ¡†æ¶ï¼Œæ¥å¤„ç†åŸºäºéšå¼åé¦ˆçš„æ¨èã€‚å¯¹æ¯”ä¼ ç»Ÿçš„æ–¹æ³•(å¦‚MFå’ŒMFçš„æ¨å¹¿ç­‰ç®—æ³•)ï¼ŒNCF(æ–‡ä¸­å®ç°äº†ä¸‰ä¸ªå®ä¾‹ï¼ŒåŒ…æ‹¬GMFã€MLPå’ŒNeuMF)èƒ½å¤Ÿæä¾›æ›´å¥½çš„æ¨èæ€§èƒ½ã€‚

#### 1. å­¦ä¹ è¿‡ç¨‹
##### 1.1 å‡†å¤‡å·¥ä½œ
æ–‡çŒ®é˜…è¯»ï¼Œä¸‹è½½çŸ¥äº‘ï¼Œåˆ›å»ºmdç¬”è®°ã€‚

ä»£ç ï¼Œå…‹éš†ä»£ç ï¼Œåˆ›å»ºcondaç¯å¢ƒï¼Œæ ¹æ®README.mdè°ƒè¯•è¿è¡Œç¯å¢ƒã€‚

##### 1.2 é˜…è¯»è®ºæ–‡
é˜…è¯»è®ºæ–‡æ–¹å¼ï¼Œä¸€å¥ä¸€å¥é˜…è¯»ï¼Œå…ˆè‡ªå·±ç¿»è¯‘ï¼Œå†çœ‹çŸ¥äº‘ç¿»è¯‘ï¼Œè§åˆ°ä¸“ä¸šåç§°å°±æŸ¥æ‰¾ç›¸å…³èµ„æ–™(æ–‡å­—æˆ–è§†é¢‘)å­¦ä¹ ï¼Œè¾¹è¯»è®ºæ–‡è¾¹åšç¬”è®°ã€‚

##### 1.3 ä»£ç è°ƒè¯•è¿è¡Œ
é˜…è¯»åˆ°è¯¥æ–‡çŒ®çš„ç¬¬ä¸‰èŠ‚(3.2, 3.3, 3.4)ï¼Œå¼€å§‹ç»“åˆè®ºæ–‡è®²è¿°çš„æ¨¡å‹æ¡†æ¶çœ‹å…·ä½“ä»£ç å®ç°ï¼Œè¿‡ç¨‹ä¸­ä¸æ–­åˆ©ç”¨debugæŸ¥çœ‹ä¸€äº›å˜é‡çš„å–å€¼å’Œæ–¹æ³•çš„å‚æ•°ã€‚

å€¼å¾—ä¸€æçš„æ˜¯ï¼Œåœ¨è°ƒè¯•ä»£ç çš„è¿‡ç¨‹ä¸­å‘ç°äº†ä¸€ä¸ªé—®é¢˜: åœ¨ä½¿ç”¨é¢„è®­ç»ƒçš„NeuMFä¸­ï¼Œä»£ç ä¸­ï¼Œä¼˜åŒ–å™¨å¹¶æ²¡æœ‰åˆ‡æ¢åˆ°SGDã€‚ç„¶åæˆ‘å°±æƒ³çœ‹ä¸€ä¸‹Adamå’ŒSGDçš„å…·ä½“æ•ˆæœåŒºåˆ«ï¼Œä½†æ˜¯ç”±äºæ•°æ®é›†å¤ªå¤§ï¼Œæ— æ³•å¿«é€Ÿè·‘å‡ºæ¨¡å‹ï¼Œæˆ‘å°±"éšæœº"åˆ‡å‡ºäº†å­æ•°æ®é›†ï¼Œå¼€å§‹äº†è®­ç»ƒã€‚(å…¶å®æˆ‘æ„Ÿè§‰äººä½œè€…å¯èƒ½åªæ˜¯ä»£ç æ²¡æäº¤ï¼Œæˆ–è€…è¿™ä¸ªåœ°æ–¹å½±å“ä¸å¤§ğŸ˜‚)

ç»è¿‡ä¸Šè¿°ä¸€ç•ª"æŠ˜è…¾"ï¼Œç®—æ˜¯æˆåŠŸåœ°æŠŠä»£ç æ‘¸ç†Ÿäº†ã€‚

#### 2. ç–‘é—®
é—®é¢˜1: è¿™ä¸ªé¢„æµ‹å› ç´ (the number of predictive factorsï¼Œä»£ç ä¸­æ˜¯num_factorsï¼Œç”¨äºEmbeddingæ—¶çš„output_dimäº†)åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ

é—®é¢˜2: åœ¨ä½¿ç”¨é¢„è®­ç»ƒçš„NeuMFä¸­ï¼Œä»£ç ä¸­ï¼Œä¼˜åŒ–å™¨å¹¶æ²¡æœ‰åˆ‡æ¢åˆ°SGD(è®ºæ–‡ä¸­åœ¨3.4.1èŠ‚è¯´è¦åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­ä½¿ç”¨SGD)ã€‚
![picture 17](assets/images/1682237389170.png)

è£å‰ªæ•°æ®é›†åï¼Œä¸‹å›¾åˆ†åˆ«æ˜¯GMFã€MLPã€NeuMF(Adam)å’ŒNeuMF(SGD)ï¼š
![picture 19](assets/images/1682322742407.png)  
![picture 20](assets/images/1682322747129.png)  
![picture 21](assets/images/1682322760186.png)  
![picture 22](assets/images/1682322766085.png)  

å¯¹äºNeuMFï¼Œä¼˜åŒ–å™¨ä»Adamæ”¹ä¸ºSGDåï¼Œè®­ç»ƒé€Ÿåº¦æœ‰æ‰€æå‡ï¼Œä½†æ˜¯HRå’ŒNDCGä¸‹é™äº†ã€‚

#### 3. æ„Ÿæ‚Ÿ
ä½œä¸ºç¬¬ä¸€ç¯‡æ–‡çŒ®ï¼Œä¸€ä¸Šæ¥æœ€ç›´æ¥çš„æ„Ÿå—å°±æ˜¯"å“‡ï¼Œå¥½å¤šä¸“ä¸šåè¯"ï¼Œæ‰€ä»¥åœ¨è¯»è®ºæ–‡çš„è¿‡ç¨‹ä¸­ä¸æ–­åœ°æŸ¥é˜…å’Œå­¦ä¹ ç›¸å…³çŸ¥è¯†ï¼Œè¿™ä¹Ÿè®©æˆ‘å¯¹è¿™ä¸ªæ–¹å‘æœ‰äº†ä¸€äº›äº†è§£ã€‚

é€šè¿‡è¯»è®ºæ–‡ï¼Œæˆ‘å‘ç°è‡ªå·±çš„æœºå™¨å­¦ä¹ ç›¸å…³åŸºç¡€çŸ¥è¯†çš„å‚¨å¤‡è¿˜ä¸å¤Ÿï¼Œæ‰€ä»¥æˆ‘æ¥ä¸‹æ¥è¦åŠ å¿«å­¦ä¹ çš„æ­¥ä¼ï¼Œæ‰“å¥½åŸºç¡€ã€‚

#### 4. é™„å½•
è®ºæ–‡çš„æ¯ä¸€ç« çš„å°ç»“ã€‚å¯¹äºç¬¬1èŠ‚INTRODUCTIONï¼Œäº†è§£äº†æœ¬æ–‡è¦åšçš„å·¥ä½œï¼Œåˆæ­¥äº†è§£äº†çŸ©é˜µåˆ†è§£MFã€‚å¯¹äºç¬¬2èŠ‚PRELIMINARIESï¼Œå®šä¹‰äº†ç”¨æˆ·-ç‰©å“äº¤äº’çŸ©é˜µï¼Œé€šè¿‡ä¸¾ä¾‹çš„æ–¹å¼ï¼Œå±•ç¤ºäº†MFçš„ç®€å•å†…ç§¯çš„å±€é™æ€§ã€‚å¯¹äºç¬¬3èŠ‚NCFï¼Œä¸»è¦ä»‹ç»äº†NCFçš„æ€»ä½“æ¡†æ¶ï¼Œè¿˜è¯æ˜äº†MFæ˜¯NCFçš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œå¹¶ä¸”å®ç°äº†ä¸‰ä¸ªNCFå®ä¾‹(åˆ†åˆ«æ˜¯GMFï¼ŒMLPå’ŒNeuMF)ã€‚å¯¹äºç¬¬4èŠ‚EXPERIMENTSï¼Œæ–‡ä¸­å…ˆæå‡ºäº†è¯•éªŒé˜¶æ®µè¦å›ç­”çš„ä¸‰ä¸ªé—®é¢˜ï¼Œå¹¶è¯´æ˜äº†å®éªŒçš„è®¾ç½®(åŒ…æ‹¬æ•°æ®é›†ï¼Œè¯„ä»·æ–¹æ³•ï¼ŒBaselineså’Œå®éªŒå‚æ•°)ï¼Œç„¶åé’ˆå¯¹æ¯ä¸ªé—®é¢˜åšäº†ç›¸å…³å®éªŒã€‚å¯¹äºç¬¬5èŠ‚æ˜¯ç›¸å…³å·¥ä½œã€‚ç¬¬6èŠ‚æ˜¯æ€»ç»“å’Œå±•æœ›ã€‚

## FREEDOM(Freezing and Denoising Graph Structures for Multimodal Recommendation)
è®¤ä¸ºLATTICEçš„æ½œåœ¨å›¾å­¦ä¹ ç»“æ„æ˜¯ä½æ•ˆä¸”ä¸å¿…è¦çš„ã€‚å®éªŒè¯æ˜ï¼Œåœ¨è®­ç»ƒä¹‹å‰å†»ç»“ç‰©å“-ç‰©å“çš„ç»“æ„ä¹Ÿå¯ä»¥è¾¾åˆ°ç›¸åŒ¹æ•Œçš„æ€§èƒ½ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæå‡ºäº†FREEDOM(FREEzes the item-item graph and DenOises the user-item interaction graph simultaneously for Multimodal recommendation)ã€‚

> Compared with LATTICE, FREEDOM achieves an average improvement of 19.07% in recommendation accuracy while reducing its memory cost up to 6Ã— on large graphs.

### æ¨¡å‹æ¡†æ¶
æ¨¡å‹å…±åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼š1.Constructing Frozen Item-Item Graphï¼›2.Denoising User-Item Bipartite Graphï¼›3.Integration of Two Graphs for Learningï¼›4.Top-K Recommendationã€‚
![picture 14](assets/images/1685345510319.png)  


#### 1.Constructing Frozen Item-Item Graph $S-Frozen$
FREEDOMä¹Ÿæ˜¯å¯¹æ¯ä¸ª**æ¨¡æ€**$m$çš„**åŸå§‹ç‰¹å¾**ä½¿ç”¨**kNN**æ„å»º**åˆå§‹æ¨¡æ€æ„ŸçŸ¥â€œç‰©å“-ç‰©å“â€å›¾**$S^m$ã€‚

1. é¦–å…ˆï¼Œ$N$ä¸ªç‰©å“ï¼Œå¯¹åˆå§‹ç‰¹å¾$x_i^m$å’Œ$x_j^m$è®¡ç®—**ä½™å¼¦ç›¸ä¼¼æ€§**ï¼Œå¦‚**å…¬å¼(1)**$S^m_{ij}=\frac{(x^m_i)^Tx^m_j}{||x^m_i||||x^m_j||}$ï¼Œå…¶ä¸­$S^m_{ij}$æ˜¯çŸ©é˜µ$S^m\in R^{N\times N}$çš„ç¬¬$i$è¡Œï¼Œç¬¬$j$åˆ—çš„å…ƒç´ ã€‚
2. å…¶æ¬¡ï¼Œä½¿ç”¨kNNç¨€ç–åŒ–ï¼Œå¹¶å°†å¸¦æƒé‡çš„çŸ©é˜µ$S^m$è½¬ä¸ºæ— æƒé‡çŸ©é˜µ$\hat S^m$ï¼Œå¦‚**å…¬å¼(2)**$\hat S^m_{ij}=\begin{cases}
    1 & S^m_{ij}âˆˆtopk(S^m_i) \\
    0 & otherwise
\end{cases}$ï¼Œå…¶ä¸­ï¼Œå°†$1$å®šä¹‰ä¸ºä¸¤ä¸ªç‰©å“$i$å’Œ$j$æœ‰**æ½œåœ¨è”ç³»**ï¼Œéœ€è¦æ³¨æ„æ˜¯$\hat S^m$ä¸åŒäº**LATTICE**ä¸­å¸¦æƒé‡çš„ç›¸ä¼¼æ€§çŸ©é˜µã€‚
3. ç„¶åï¼ŒåŒæ ·åœ°å¯¹çŸ©é˜µ$\hat S^m$å½’ä¸€åŒ–ï¼Œå¾—åˆ°çŸ©é˜µ$\widetilde{S}^m=(D^m)^{-\frac{1}{2}}\hat{S}^m(D^m)^{-\frac{1}{2}}$ã€‚
4. å†ç„¶åï¼Œå¯¹æ¯ä¸ªæ¨¡æ€çš„$\widetilde{S}^m$è¿›è¡Œæ•´åˆï¼Œå¦‚**å…¬å¼(3)**$S=\sum\limits_{m\in M}\alpha_m\widetilde{S}^m$ï¼Œå…¶ä¸­ï¼Œ$\alpha_m$è¡¨ç¤º**æ¨¡æ€**$m$çš„**é‡è¦æ€§è¯„åˆ†**ï¼Œ$M$æ˜¯**æ¨¡æ€é›†**ã€‚åœ¨æ­¤ï¼Œå®šä¹‰æ¨¡æ€é›†$M=\{v,t\}$ï¼Œå®šä¹‰è¶…å‚æ•°$\alpha_v$è¡¨ç¤ºè§†è§‰æ¨¡æ€çš„é‡è¦æ€§ï¼Œå¹¶ä»¤$\alpha_t=1-\alpha_v$ã€‚
5. æœ€åï¼Œ**å†»ç»“æ½œåœ¨ç‰©å“-ç‰©å“å›¾**$S$å¾—åˆ°$S-Frozen$ã€‚

#### 2.Denoising User-Item Bipartite Graph $\hat A_\rho$
å®šä¹‰**ç”¨æˆ·-ç‰©å“å›¾**$G=(\upsilon, \varepsilon)$ï¼Œå…¶ä¸­ï¼Œ$\upsilon$ä¸ºé¡¶ç‚¹é›†ï¼Œ$\varepsilon$ä¸ºè¾¹é›†ï¼Œ$M$å’Œ$N$åˆ†åˆ«ä¸ºç”¨æˆ·æ•°é‡å’Œç‰©å“æ•°é‡ã€‚$M+N=|\upsilon|$ä¸ºå®šç‚¹æ•°é‡ã€‚

1. é¦–å…ˆï¼Œåˆ©ç”¨**ç”¨æˆ·-ç‰©å“äº¤äº’çŸ©é˜µ**$R\in R^{M\times N}$æ„å»º**å¯¹ç§°é‚»æ¥çŸ©é˜µ**$A\in R^{|\upsilon|\times |\upsilon|}$ï¼Œå¦‚**å…¬å¼(4)**$A=\begin{pmatrix}
    0&R \\
    R^T&0
\end{pmatrix}$ï¼Œå…¶ä¸­$A_{ui}=\begin{cases}
    1 & user\ u\ has\ interacted\ with\ item\ i \\
    0 & otherwise
\end{cases}$ã€‚
2. å†è€…ï¼Œä»¿ç…§**DropEdge**ï¼Œå¯¹çŸ©é˜µ$A$è¿›è¡Œä¿®å‰ªï¼Œå¾—åˆ°çŸ©é˜µ$A_\rho$ï¼Œå…¶ä¸­$\rho$æ˜¯å›¾çš„è¾¹çš„ä¿®å‰ªæ¯”ä¾‹ã€‚
3. æœ€åï¼Œå¯¹çŸ©é˜µ$A_\rho$å½’ä¸€åŒ–ï¼Œå¾—åˆ°$\hat A_\rho$ã€‚

#### 3.Integration of Two Graphs for Learning
1. é¦–å…ˆï¼Œå¯¹**ç‰©å“-ç‰©å“å›¾**$S$è¿›è¡Œ**å›¾å·ç§¯**ï¼Œå–æœ€åä¸€å±‚å·ç§¯ç»“æœä¸º$\widetilde{h}_i$ã€‚
2. åŒæ ·åœ°ï¼Œå¯¹**ç”¨æˆ·-ç‰©å“å›¾**$\hat A_\rho$è¿›è¡Œ**å›¾å·ç§¯**å’Œ**READOUT**ï¼Œå¾—åˆ°$\hat h_u \in R^d$å’Œ$\hat h_i \in R^d$ã€‚
3. æœ€åï¼Œåˆ©ç”¨**å…¬å¼(8)**$h_u=\hat h_u$ï¼Œ$h_i=\widetilde{h}_i+\hat h_i$ï¼Œå¾—åˆ°**ç”¨æˆ·-ç‰©å“å›¾**å’Œ**ç‰©å“-ç‰©å“å›¾**çš„æœ€ç»ˆè¡¨ç¤º$h_u$å’Œ$h_i$ã€‚
4. å¦å¤–ï¼Œå¯¹**åˆå§‹ç‰¹å¾**åˆ©ç”¨**MLPs**è®¡ç®—äº†**ç‰©å“çš„å•æ¨¡æ€è¡¨ç¤º**ï¼Œå¦‚**å…¬å¼(9)**$h_i^m=x_i^mW_m+b_m$ã€‚
5. æ¨¡å‹ä¼˜åŒ–ï¼Œé‡‡ç”¨**the pairwise Bayesian personalized ranking (BPR) loss**ï¼Œå¦‚**å…¬å¼(10)**$L_{bpr}=\sum \limits_{(u,i,j)\in D}(-\log \sigma(h_u^Th_i-h_u^Th_j)+\lambda\sum\limits_{m\in M}-\log \sigma(h_u^Th_i^m-h_u^Th_j^m))$ã€‚

#### 4.Top-K Recommendation

### å®éªŒ
å®éªŒå›ç­”ä»¥ä¸‹ä¸‰ä¸ªé—®é¢˜ï¼š
- How does FREEDOM perform compared with the state-of-the-art methods for recommendation? As our model improves LATTICE by freezing and denoising the graph structures, how about its improvement over LATTICE?
- How efficient of our proposed FREEDOM in terms of computational complexity and memory cost?
- How do different components in FREEDOM  influence its recommendation accuracy?
- How sensitive is our model under the perturbation of hyperparameters?

#### å®éªŒè®¾ç½®
**æ•°æ®é›†**ä½¿ç”¨**Clothing**, **Sports**, and **Baby**ã€‚**æ¨¡æ€**æœ‰**visual**å’Œ**textual**ä¸¤ç§ã€‚
![picture 9](assets/images/1685259580643.png)  

#### Baselines
ä½¿ç”¨**é€šç”¨CFæ¨èæ¨¡å‹**å’Œ**å¤šæ¨¡æ€æ¨èæ¨¡å‹**ã€‚
- BPRï¼ŒCFæ¨¡å‹
- LightGCNï¼ŒCFæ¨¡å‹
- VBPRï¼Œä»¥ä¸‹éƒ½æ˜¯å¤šæ¨¡æ€æ¨èæ¨¡å‹
- GRCN
- DualGNN
- LATTICE
- SLMRec

#### å®éªŒç»“æœ
![picture 10](assets/images/1685259927365.png)  
![picture 11](assets/images/1685259940289.png)  
![picture 12](assets/images/1685259948749.png)  
![picture 13](assets/images/1685259961343.png)  

### ç»“è®º
> In this paper, we experimentally reveal that the graph structure learning in a state-of-the-art multimodal recommendation model (i.e. LATTICE) plays a trivial role in its performance. It is the item-item graph constructed from raw multimodal features that contributes to the recommendation accuracy. Based on the finding, we propose a model that freezes the item-item graph and denoises the user-item graph simultaneously for multimodal recommendation. In denoising, we devise a degree-sensitive edge pruning method to sample the user-item graph, which shows better performance than the random edge dropout for recommendation. Finally, we conduct extensive experiments to demonstrate the proposed model not only outperforms the baselines with a large margin but also can reduce the memory cost of LATTICE by 6Ã— on large graphs.

## LATTICE(Mining Latent Structures for Multimedia Recommendation)
å…ˆå‰çš„å·¥ä½œæ˜¯ä½¿ç”¨**å¤šæ¨¡æ€ç‰¹å¾**ä½œä¸º**å‰¯ä¿¡æ¯**æ¥å¯¹**â€œç”¨æˆ·-ç‰©å“â€äº¤äº’**å»ºæ¨¡ï¼Œä½†æ˜¯è¿™ç§æ–¹å¼ä¸é€‚åˆæ¨èç³»ç»Ÿã€‚å…·ä½“æ¥è¯´ï¼Œåªæ˜¯é€šè¿‡**é«˜é˜¶â€œç‰©å“-ç”¨æˆ·-ç‰©å“â€å…³ç³»**æ¥éšå¼åœ°å»ºæ¨¡**ååŒâ€œç‰©å“-ç‰©å“â€å…³ç³»**ã€‚

> The majority of previous work focuses on modeling **user-item interactions** with **multimodal features** included as **side information**. However, this scheme is not well-designed for multimedia recommendation. Specifically, only **collaborative item-item relationships** are implicitly modeled through **high-order item-user-item relations**.

### æ¨¡å‹æ¡†æ¶
$U$å’Œ$I$ä¸ºç”¨æˆ·é›†å’Œç‰©å“é›†ï¼Œ$u$è¡¨ç¤ºä¸€ä¸ªç”¨æˆ·ï¼Œ$uâˆˆU$ã€‚å¦‚æœç”¨æˆ·$u$ä¸ç‰©å“$i$æœ‰å…³è”ï¼Œé‚£ä¹ˆè¯´æœ‰æ­£åé¦ˆï¼Œç”¨$y_{ui}=1$è¡¨ç¤ºï¼Œå…¶ä¸­$iâˆˆI^u$ã€‚ç”¨$x_u,x_iâˆˆR^d$è¡¨ç¤ºæŸä¸ªç”¨æˆ·å’Œç‰©å“çš„è¾“å…¥IDåµŒå…¥ï¼Œå…¶ä¸­$d$è¡¨ç¤ºåµŒå…¥å±‚çš„ç»´åº¦ã€‚ç”¨$e^m_iâˆˆR^{d_m}$è¡¨ç¤ºç‰©å“$i$çš„æŸä¸ªæ¨¡æ€çš„ç‰¹å¾ï¼Œå…¶ä¸­$d_m$è¡¨ç¤ºæŸä¸ªæ¨¡ç‰¹ä¸‹çš„ç‰¹å¾çš„ç»´åº¦ï¼Œç”¨$mâˆˆM$è¡¨ç¤ºæ¨¡æ€ï¼Œ$M$è¡¨ç¤ºæ¨¡æ€é›†ã€‚

æ¨¡å‹å…±åˆ†ä¸º3ä¸ªéƒ¨åˆ†ï¼š1.Modality-aware Latent Structure Learningï¼Œ2.Graph Convolutionså’Œ3.Combining with Collaborative Filteringã€‚å¦‚ä¸‹å›¾ã€‚
![picture 1](assets/images/1685004655739.png)

#### 1. Modality-aware Latent Structure Learning
##### ç¬¬ä¸€æ­¥ï¼ŒConstructing initial ğ‘˜NN modality-aware graphs $\widetilde{S}^m$
1. é¦–å…ˆï¼Œå¯¹äº$e^m_i$ï¼Œè®¡ç®—ä¸å¦ä¸€ç‰¹å¾$e^m_j$ä¹‹é—´çš„**ä½™å¼¦ç›¸ä¼¼åº¦**ï¼Œå¾—åˆ°å›¾é‚»æ¥çŸ©é˜µ$S^m_{ij}$ï¼Œå¦‚ä¸‹**å…¬å¼(1)**$S^m_{ij}=\frac{(e^m_i)^Te^m_j}{||e^m_i||||e^m_j||}$ã€‚
2. å…¶æ¬¡ï¼Œæ§åˆ¶é‚»æ¥çŸ©é˜µçš„å…ƒç´ éè´Ÿï¼ŒèŒƒå›´ä¸º$[0,1]$ã€‚
3. ç„¶åï¼Œåˆ©ç”¨`kNN`ç¨€ç–åŒ–é‚»æ¥çŸ©é˜µï¼Œå¦‚ä¸‹**å…¬å¼(2)**$\hat{S^m_{ij}}=\begin{cases}
    S^m_{ij} & S^m_{ij}âˆˆtopk(S^m_i) \\
    0 & otherwise
\end{cases}$ï¼Œå…¶ä¸­ç»“æœ$\hat{S^m}$æ˜¯**ç¨€ç–åŒ–çš„æœ‰å‘å›¾é‚»æ¥çŸ©é˜µ**ã€‚
4. æœ€åï¼Œå¯¹$\hat{S}^m$è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¦‚ä¸‹**å…¬å¼(3)**$\widetilde{S}^m=(D^m)^{-\frac{1}{2}}\hat{S}^m(D^m)^{-\frac{1}{2}}$ï¼Œå…¶ä¸­$D^m$è¡¨ç¤º$\hat{S}^m$çš„**å¯¹è§’åº¦çŸ©é˜µ**ï¼Œå…¶è®¡ç®—æ–¹æ³•ä¸º$D^m_{ii}=\sum_{j}\hat{S}^m_{ij}$ï¼Œç”±æ­¤å¯ä»¥çœ‹å‡º$D^m$åªæœ‰å¯¹è§’çº¿å…ƒç´ ï¼Œå…ƒç´ å€¼ä¸º$\hat{S}^m$çš„ç¬¬$i$è¡Œå…ƒç´ ä¹‹å’Œã€‚

##### ç¬¬äºŒæ­¥ï¼ŒLearning latent structures $A^m$
1. é¦–å…ˆï¼Œå¯¹äº$e^m_i$ï¼Œè®¡ç®—**high-level feature vector**ï¼Œå¦‚ä¸‹**å…¬å¼(4)**$\widetilde{e}^m_i=W_me^m_i+b_m$ï¼Œå…¶ä¸­ï¼Œ$W_m \in R^{d'\times d_m}$å®šä¹‰ä¸º**trainable transformation matrix**ï¼Œ$b_m \in R^{d'}$ã€‚
2. ç„¶åï¼Œå¯¹$\widetilde{e}^m_i$æ ¹æ®å…¬å¼(1)(2)(3)è®¡ç®—å‡ºé‚»æ¥çŸ©é˜µ$\widetilde{A}^m$ã€‚
3. æœ€åï¼Œæ ¹æ®**å…¬å¼(5)**$A^m=\lambda \widetilde{S}^m+(1-\lambda) \widetilde{A}^m$ï¼Œå¾—å‡ºæœ€ç»ˆçš„**å›¾é‚»æ¥çŸ©é˜µ**ï¼Œå…¶ä¸­ï¼Œç³»æ•°$\lambda \in (0,1)$ï¼Œ$\widetilde{S}^m$ä¸ºä¸Šä¸€æ­¥æœ€åçš„è®¡ç®—ç»“æœã€‚

##### ç¬¬ä¸‰æ­¥ï¼ŒAggregating multimodal latent graphs å¾—åˆ° $A$
1. é¦–å…ˆï¼Œåˆ©ç”¨**å…¬å¼(6)**$A=\sum_{m=0}^{|M|}a_mA^m$è®¡ç®—å‡ºæœ€ç»ˆçš„**Latent Structure**$A$ï¼Œå…¶ä¸­ï¼Œ$a_m$æ˜¯æ¨¡æ€$m$çš„é‡è¦åº¦è¯„åˆ†ï¼Œ$A$æ˜¯è¡¨ç¤ºå¤šä¸ªæ¨¡æ€çš„ç‰©å“å…³ç³»çš„å›¾ç»“æ„ã€‚
2. æœ€åï¼Œä½¿ç”¨**softmax function**è®©$A$å½’ä¸€åŒ–ï¼Œå¹¶ä½¿$\sum_{m=0}^{|M|}a_m=1$ã€‚

#### 2. Graph Convolutions
åˆ©ç”¨**å…¬å¼(7)**$h_i^{(l)}=\sum \limits_{j\in N(i)}A_{ij}h_j^{(l-1)}$è¿›è¡Œå›¾å·ç§¯ï¼Œå…¶ä¸­$N(i)$è¡¨ç¤ºç‰©å“$i$çš„é‚»æ¥ç‰©å“ï¼Œ$h_i^{(l)}\in R^d$è¡¨ç¤ºç¬¬$l$å±‚çš„ç‰©å“$i$çš„è¡¨ç¤ºã€‚

> We set the input item representation $h_i^{(0)}$ as its corresponding ID embedding vector $x_i$.
> After stacking $L$ layers, $h_i^{(L)}$ encodes the high-order item-item relationships that are constructed by multimodal information and thus can benefit the downstream CF methods.

#### 3. Combining with Collaborative Filtering
åœ¨ä»»ä½•CFæ–¹æ³•ä¹‹å‰ä½¿ç”¨ä»¥ä¸Šæ­¥éª¤ã€‚

å°†$\widetilde{x}_u,\widetilde{x}_i\in R^d$å®šä¹‰ä¸ºCFæ–¹æ³•çš„ç”¨æˆ·å’Œç‰©å“åµŒå…¥çš„è¾“å‡ºã€‚ç„¶åï¼Œé€šè¿‡å¢åŠ ä»ç‰©å“å›¾ä¸­å­¦ä¹ åˆ°çš„å½’ä¸€åŒ–çš„ç‰©å“åµŒå…¥$h_i^{(L)}$å¢å¼ºç‰©å“åµŒå…¥ï¼Œå¦‚**å…¬å¼(8)**$\hat x_i=\widetilde{x}_i+\frac{h_i^{(L)}}{||h_i^{(L)}||_2}$ã€‚æœ€åï¼Œè®¡ç®—**ç”¨æˆ·-ç‰©å“åå¥½å¾—åˆ†**(the user-item preference score)ï¼Œå¦‚**å…¬å¼(9)**$\hat y_{ui}=\widetilde x_u^T \hat x_i$ã€‚

#### 4. Optimization
ä½¿ç”¨**Bayesian Personalized Ranking (BPR) loss**ã€‚

### å®éªŒ
å®éªŒå›ç­”ä»¥ä¸‹ä¸‰ä¸ªé—®é¢˜ï¼š
- How does our model perform compared with the state-of-the-art multimedia recommendation methods and other CF methods in both warm-start and cold-start settings?
- How effective are the item graph structures learned from multimodal features?
- How sensitive is our model under the perturbation of several key hyper-parameters?

#### å®éªŒè®¾ç½®
**æ•°æ®é›†**ä½¿ç”¨**Clothing**, **Sports**, and **Baby**ã€‚**æ¨¡æ€**æœ‰**visual**å’Œ**textual**ä¸¤ç§ã€‚
![picture 4](assets/images/1685063920356.png)  

#### Baselines
- MF
- NGCF
- LightGCN
- VBPR
- MMGCN
- GRCN
  
#### å®éªŒç»“æœ
![picture 5](assets/images/1685063957784.png)  
***
![picture 6](assets/images/1685063970282.png) 
***
![picture 7](assets/images/1685063990607.png)
***
![picture 8](assets/images/1685064001159.png)  

### ç»“è®º
> In this paper, we have proposed **the latent structure mining method**(LATTICE) for multimodal recommendation, which leverages **graph structure learning** to discover **latent item relationships** underlying multimodal features. In particular, we first devise **a modality-aware graph structure learning layer** that learns item graph structures from multimodal features and fuses multimodal graphs. Along **the learned graph structures**, one item can receive **informative high-order affinities** from **its neighbors** by **graph convolutions**. Finally, we combine **our model** with **downstream CF methods** to make recommendations. Empirical results on three public datasets demonstrate the effectiveness of our proposed model.

## GETNext(GETNext: Trajectory Flow Map Enhanced Transformer for Next POI Recommendation)
**Next PoI Recommendation**ç”¨äºæ ¹æ®ç”¨æˆ·å½“å‰çŠ¶æ€å’Œå†å²ä¿¡æ¯æ¥é¢„æµ‹ç”¨æˆ·çš„**immediate future movements**ã€‚è¿™ä¸ªé—®é¢˜éœ€è¦è€ƒè™‘å„ç§æ•°æ®çš„è¶‹åŠ¿ï¼ˆå¦‚ç©ºé—´ä½ç½®ã€æ—¶é—´èƒŒæ™¯å’Œç”¨æˆ·åå¥½ç­‰ï¼‰ã€‚

ç°æœ‰æ–¹æ³•æŠŠ**Next PoI Recommendation**è§†ä¸º**åºåˆ—é¢„æµ‹é—®é¢˜**ï¼Œå¿½ç•¥äº†æ¥è‡ª**å…¶ä»–ç”¨æˆ·**çš„**åä½œä¿¡å·**ã€‚ç°æœ‰æ–¹æ³•æœ‰ä¸‰ä¸ªä¸è¶³ç‚¹ï¼š1ï¼‰å¯¹æ¯”é•¿è½¨è¿¹ï¼Œåœ¨çŸ­è½¨è¿¹ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼›2ï¼‰å¯¹é‚£äº›éæ´»è·ƒç”¨æˆ·çš„æ¨èç²¾ç¡®åº¦ä½ï¼›3ï¼‰æ— æ³•å»ºç«‹æ—¶é—´å’ŒPOIç±»åˆ«é—´çš„æ¡¥æ¢ã€‚

æœ¬æ–‡æå‡ºäº†**user-agnostic global trajectory flow map**å’Œ**GETNext**æ¨¡å‹ï¼Œå¯ä»¥æ›´å¥½åœ°åˆ©ç”¨**åä½œä¿¡å·**æ¥åšæ›´ç²¾ç¡®çš„**Next POI Recommendation**ã€‚åŒæ—¶ï¼Œç¼“è§£äº†å†·å¯åŠ¨é—®é¢˜ã€‚

æ–‡ä¸­æå‡ºä¸‰ä¸ªé—®é¢˜ï¼š
1. How to aggregate information from check-in sequences to form a unified representation of global trajectory flow patterns?
æ„å»ºäº†ä¸€ä¸ª**user-agnostic trajectory flow map**ã€‚ä½¿ç”¨**å›¾å·ç§¯ç½‘ç»œ**å°†**POI**åµŒå…¥åˆ°**æ½œåœ¨ç©ºé—´**ä¸­ï¼Œä½¿ä¹‹ä¿æŒ**POIé—´çš„å…¨å±€è¿‡æ¸¡**ï¼ˆthe global transitions among POIsï¼‰ã€‚

2. How to reserve the important spatio-temporal contextual information such as category information and user preference besides these trajectory flows?
åˆ©ç”¨**the embedding layers**æ•è·**ç”¨æˆ·çš„æ€»ä½“åå¥½**ã€**POIç±»åˆ«åµŒå…¥**ã€‚åˆ©ç”¨**a time2vec model**æ¥æè¿°**æ—¶é—´åµŒå…¥**ã€‚ä¸ºäº†è¿æ¥POIç±»åˆ«å’Œæ—¶é—´ï¼Œåˆ©ç”¨**a fusion module**åˆå¹¶**POIç±»åˆ«åµŒå…¥**å’Œ**æ—¶é—´åµŒå…¥**ï¼Œå¾—åˆ°**the time-aware category context embedding**ã€‚

3. How to leverage all information above in next POI recommendation, to strike a balance between generic movement patterns and personalized demands?
åˆ©ç”¨**transformer**å’Œ**å‡ ä¸ªMLP**ã€‚

### é—®é¢˜å…¬å¼åŒ–(PROBLEM FORMULATION)
- è®¾ç½®**ç”¨æˆ·é›†**ä¸º$U=\{u_1,u_2,...,u_M\}$
- è®¾ç½®**POIsé›†**ä¸º$P=\{p_1,p_2,...,p_N\}$ï¼Œä¾‹å¦‚ç‰¹å®šçš„é¤å…ã€é…’åº—ç­‰ã€‚
- è®¾ç½®**æ—¶é—´æˆ³é›†**ä¸º$T=\{t_1,t_2,...,t_K\}$ã€‚
- ä»¥ä¸Šçš„$M,N,K$å‡ä¸º**æ­£æ•´æ•°**ã€‚
- æ¯ä¸ª**POI** $p\in P$ å®šä¹‰ä¸ºä¸€ä¸ª**å…ƒç»„** $p=\lang lat,lon,cat,freq\rang$ï¼Œå…¶ä¸­$lat,lon,cat,freq$åˆ†åˆ«è¡¨ç¤º**çº¬åº¦**ï¼Œ**ç»åº¦**ï¼Œ**ç±»åˆ«**å’Œ**check-inåºåˆ—**ã€‚ç‰¹åˆ«åœ°ï¼Œ$cat$ï¼ˆç±»åˆ«ï¼‰æ˜¯å–è‡ª**POIç±»åˆ«**çš„å›ºå®šåˆ—è¡¨ï¼ˆå¦‚â€œç«è½¦ç«™â€ï¼Œâ€œé…’å§â€ï¼‰ã€‚
- å®šä¹‰**check-in**ä¸ºä¸€ä¸ª**å…ƒç»„** $q=\lang u,p,t\rang\in U\times P\times T$ï¼Œè¡¨ç¤ºæŸä¸ªç”¨æˆ·$u$åœ¨æŸä¸ªæ—¶é—´æˆ³$t$å»äº†æŸä¸ªPOI $p$ã€‚
- å®šä¹‰æŸä¸ª**ç”¨æˆ·**$u\in U$çš„**check-inåºåˆ—**ä¸º$Q_u=(q_u^1,q_u^2,q_u^3,...)$ï¼Œå…¶ä¸­$q_u^i$è¡¨ç¤ºç¬¬$i$ä¸ª**check-in**è®°å½•ã€‚**æ‰€æœ‰ç”¨æˆ·**çš„**check-inåºåˆ—**ä¸º$Q_U=\{Q_{u_1},Q_{u_2},...,Q_{u_M}\}$ã€‚
- **æ•°æ®é¢„å¤„ç†**ï¼Œå°†ä»»æ„ç”¨æˆ·$u$çš„**check-inåºåˆ—**$Q_u$æ‹†åˆ†ä¸º**ä¸€ç»„è¿ç»­è½¨è¿¹**$Q_u=S_u^1\oplus S_u^2\oplus \cdot\cdot\cdot$ï¼Œå…¶ä¸­ï¼Œ$\oplus$å®šä¹‰ä¸º**ä¸²è”**æ“ä½œï¼Œ$S_u^i$ä¸ºæŸ**ç”¨æˆ·**$u$çš„**å†å²è½¨è¿¹**é›†åˆã€‚
- **Next POI Recommendation**çš„ç›®æ ‡æ˜¯ï¼šå­¦ä¹ ç»™å‡ºçš„**å†å²è½¨è¿¹é›†åˆ**$\mathcal{S}=\{S_u^i\}_{i\in \mathbb{N},u\in U}$å’Œ**å½“å‰è½¨è¿¹**$S^\prime=(q_1,q_2,...,q_m)$ï¼Œé¢„æµ‹æœªæ¥ç”¨æˆ·$u$æœ€æœ‰å¯èƒ½è®¿é—®çš„**POIs**ï¼ˆ$q_{m+1},q_{m+2},...,q_{m+k}$ï¼‰ï¼Œå…¶ä¸­ï¼Œ$k\geq 1$æ˜¯**å°æ•´æ•°**ï¼Œé€šå¸¸$k=1$ã€‚

### æ¨¡å‹æ¡†æ¶ GETNext
ä¸‹å›¾ä¸ºGETNextæ¨¡å‹ï¼Œæ¨¡å‹èåˆäº†å‡ ä¸ªå…³é”®çš„æ¨¡å—ã€‚
![picture 15](assets/images/1685581695661.png)

#### Learning with Trajectory Flow Map
åœ¨æœ¬èŠ‚ä¸­ï¼Œå®šä¹‰äº†**trajectory flow map**ï¼Œå¹¶åˆ©ç”¨å®ƒç”Ÿæˆäº†**POI Embedding**ï¼ˆç¼–ç äº†æ¯ä¸ªPOIçš„`users' generic movement patterns`ï¼Œå¹¶ä¸”åˆå¹¶äº†æ¯ä¸ªPOIçš„ç±»åˆ«ã€ä½ç½®å’Œcheck-iné¢‘ç‡ï¼‰å’Œ**Transition Attention Map**ï¼ˆæ¨¡æ‹Ÿäº†POIsä¹‹é—´çš„`transition probabilities`ï¼‰ã€‚

##### POI Embedding
åœ¨**trajectory flow map**ä¸Šè®­ç»ƒ**GNN(Graph Neural Network)**ç”Ÿæˆ**POI Embedding**ã€‚

å®šä¹‰**Trajectory Flow Map**æ˜¯ä¸€ä¸ª**å¸¦æœ‰å±æ€§çš„åŠ æƒæœ‰å‘å›¾**$\mathcal{G}=(V,E,l,w)$ã€‚
- **èŠ‚ç‚¹é›†**$V=$ **POIsé›†**$P$ã€‚
- $\ell(p)$è¡¨ç¤º**å±æ€§**ã€‚
- $E$è¡¨ç¤º**è¾¹é›†**ã€‚è‹¥$(p_1,p_2)$å‡ºç°åœ¨**ä¸€ä¸ªè½¨è¿¹**$S_u^i$ä¸­ï¼Œåˆ™å­˜åœ¨**ä¸€æ¡è¾¹**ä»$p_1$åˆ°$p_2$ï¼Œå³å®ƒä»¬å¯è¢«è¿ç»­è®¿é—®ã€‚
- $w(p_1,p_2)$æ˜¯**æƒé‡**ï¼Œè¡¨ç¤º**è¾¹**$(p_1,p_2)$å‡ºç°åœ¨**å†å²è½¨è¿¹**$\mathcal{S}$ä¸­çš„**æ¬¡æ•°**ã€‚

åˆ©ç”¨**å›¾å·ç§¯ç½‘ç»œ**å’Œ$\mathcal{G}$ç”Ÿæˆ**POI EmbeddingçŸ©é˜µ**ï¼Œåˆ†ä¸ºä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ã€‚

é¦–å…ˆï¼Œè®¡ç®—**å½’ä¸€åŒ–çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ**ï¼Œå¦‚**å…¬å¼(1)**$\widetilde{\mathbf{L}}=(\mathbf{D}+\mathbf{I}_N)^{-1}(\mathbf{A}+\mathbf{I}_N)$ï¼Œå…¶ä¸­ï¼Œ$\mathbf{A}\in\mathbb{R}^{N\times N}$å®šä¹‰ä¸º$\mathcal{G}$çš„é‚»æ¥çŸ©é˜µï¼Œ$\mathbf{D}$ä¸º$\mathcal{G}$çš„åº¦çŸ©é˜µï¼Œ$\mathbf{I}_N$ä¸º$\mathcal{G}$çš„çš„å•ä½çŸ©é˜µã€‚
``` python
# in train.py
raw_A = load_graph_adj_mtx(args.data_adj_mtx) # Gçš„é‚»æ¥çŸ©é˜µA in å…¬å¼(1)
A = calculate_laplacian_matrix(raw_A, mat_type='hat_rw_normd_lap_mat') # å…¬å¼(1) è®¡ç®—æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ
```
ç„¶åï¼Œå®šä¹‰$\mathbf{H}^{(0)}=\mathbf{X}\in\mathbb{R}^{N\times C}$ä¸º**è¾“å…¥èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ**ã€‚å®šä¹‰**GCNå±‚ä¹‹é—´**çš„**ä¼ æ’­è§„åˆ™**ä¸º**å…¬å¼(2)**$\mathbf{H}^{(l)}=\sigma(\widetilde{\mathbf{L}}\mathbf{H}^{(l-1)}\mathbf{W}^{(l)}+\mathbf{b}^{(l)})$ï¼Œå…¶ä¸­ï¼Œ$\mathbf{H}^{(l-1)}$å®šä¹‰ä¸ºç¬¬$l$å±‚çš„è¾“å…¥ä¿¡å·ï¼ˆ$l>0$ï¼‰ï¼Œ$\mathbf{W}^{(l)}\in\mathbb{R}^{C\times\Omega}$è¡¨ç¤ºç¬¬$l$å±‚çš„**æƒé‡çŸ©é˜µ**ï¼Œ$b^{(l)}\in\mathbb{R}^{C\times\Omega}$è¡¨ç¤ºç›¸å…³åç½®ï¼Œ$\sigma$è¡¨ç¤º`leaky ReLU`æ¿€æ´»å‡½æ•°ï¼ˆ`leaky rate`ä¸º0.2ï¼‰ã€‚

æœ€åï¼Œå †å äº†$l^*$ä¸ªGCNå±‚ï¼Œåœ¨æœ€åä¸€å±‚ä¹‹å‰ä½¿ç”¨`Dropout`ï¼ŒGCNæ¨¡å‹çš„è¾“å‡ºå¯ä»¥è¡¨ç¤ºä¸º**å…¬å¼(3)**$e_\mathbf{P}=\widetilde{\mathbf{L}}\mathbf{H}^{(l^*)}\mathbf{W}^{(l^*+1)}+\mathbf{b}^{(l^*+1)}\in\mathbb{R}^{N\times\Omega}$ï¼Œå…¶ä¸­ï¼ŒPOI$p_i$çš„åµŒå…¥$e_{p_i}$æ˜¯çŸ©é˜µ$e_\mathbf{P}\in\mathbb{R}^{N\times\Omega}$çš„ç¬¬$i$è¡Œã€‚
``` python
# class GCN in model.py
# def GCN init
def __init__(self, ninput, nhid, noutput, dropout):
    super(GCN, self).__init__()

    self.gcn = nn.ModuleList()
    self.dropout = dropout
    self.leaky_relu = nn.LeakyReLU(0.2)

    channels = [ninput] + nhid + [noutput]
    for i in range(len(channels) - 1):
        gcn_layer = GraphConvolution(channels[i], channels[i + 1])
        self.gcn.append(gcn_layer)
# def GCN forward
def forward(self, x, adj):
    for i in range(len(self.gcn) - 1):
        x = self.leaky_relu(self.gcn[i](x, adj)) # å…¬å¼(2)

    x = F.dropout(x, self.dropout, training=self.training) # æœ€åä¸€å±‚å‰ä½¿ç”¨Dropout
    x = self.gcn[-1](x, adj) # å…¬å¼(3)

    return x
```
> **POI Embeddingçš„ä½œç”¨**
> Loosely speaking, the embedding of a POI $p$ indicates **the position** of $p$ within **the historical trajectories of all users** and thus captures **generic movement patterns** at $p$. It will be in turn fed to **the transformer downstream** to model **usersâ€™ visiting behaviors**. Note that even when **the current trajectory is short**, the POI embeddings nevertheless provides **rich information** to the prediction model.

> 1.ä¸ºä»€ä¹ˆPOI Embeddingä¼šè¡¨ç¤ºæ‰€æœ‰ç”¨æˆ·åœ¨å†å²è½¨è¿¹ä¸­çš„POIä½ç½®å‘€ï¼Ÿ
> 2.ä¸ºä»€ä¹ˆæ˜¯åªéšå¼åœ°æ•è·å›¾$\mathcal{G}$çš„`generic movement patterns`ï¼Ÿ

##### Transition Attention Map
ä¸ºäº†æ”¾å¤§**é›†ä½“ä¿¡å·**ï¼ˆ`the collection signals`ï¼‰çš„å½±å“ï¼Œæå‡ºäº†`Transition Attention Map`æ¥æ˜¾å¼åœ°æ¨¡æ‹Ÿä»ä¸€ä¸ªPOIåˆ°å¦ä¸€ä¸ªçš„**è½¬ç§»æ¦‚ç‡**ï¼ˆ`the transition probabilities`ï¼‰ã€‚è¿™äº›**è½¬ç§»æ¦‚ç‡**ç”¨äºè°ƒæ•´**æœ€ç»ˆçš„POIé¢„æµ‹**ã€‚

ç»™å‡ºè¾“å…¥èŠ‚ç‚¹ç‰¹å¾$\mathrm{X}$å’Œå›¾$\mathcal{G}$ï¼Œè®¡ç®—æ³¨æ„åŠ›å›¾$\Phi$ã€‚å¦‚**å…¬å¼(4)**$\Phi_1=(\mathrm{X}\times\mathrm{W}_1)\times\alpha_1\in\mathbb{R}^{N\times 1}$ï¼Œ**å…¬å¼(5)**$\Phi_2=(\mathrm{X}\times\mathrm{W}_2)\times\alpha_2\in\mathbb{R}^{N\times 1}$å’Œ**å…¬å¼(6)**$\Phi=(\Phi_1\times 1^T+1\times\Phi_1^T)\odot(\widetilde{\mathrm{L}}+J_N)\in\mathbb{R}^{N\times N}$ï¼Œå…¶ä¸­$\mathrm{W}_1,\mathrm{W}_2\in\mathbb{R}^{C\times h}$è¡¨ç¤ºå¯è®­ç»ƒçš„**ç‰¹å¾å˜æ¢çŸ©é˜µ**ï¼Œ$\alpha_1,\alpha_2\in\mathbb{R}^{h}$è¡¨ç¤ºå¯è®­ç»ƒå‘é‡ç”¨äºåå‘ä¼ æ’­æ„å»º**æ³¨æ„çŸ©é˜µ**ï¼Œ$1\in\mathbb{R}^{N\times 1}$æ˜¯ä¸€ä¸ªå…¨ä¸ºä¸€çš„**å‘é‡**ï¼Œ$J_N$è¡¨ç¤ºå…ƒç´ å…¨ä¸ºä¸€çš„**çŸ©é˜µ**ï¼Œ$\odot$è¡¨ç¤ºä¸¤ä¸ªçŸ©é˜µ**é€ä¸ªå…ƒç´ ç›¸ä¹˜**ï¼Œå¹¶å°†**æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ**$\widetilde{\mathrm{L}}$çš„å–å€¼èŒƒå›´ä»$[0,1]$æ”¹ä¸º$[1,2]$ï¼ˆä¸ºäº†é¿å…**é›¶å€¼**ï¼‰ã€‚
``` python
# def class NodeAttnMap in model.py
# in train.py
node_attn_model = NodeAttnMap(in_features=X.shape[1], nhid=args.node_attn_nhid, use_mask=False) # å…¬å¼(4,5,6)
# 'get transition attention map' in methon adjust_pred_prob_by_graph
attn_map = node_attn_model(X, A)
```

`Transition Attention Map`$\Phi$çš„ç¬¬$i$è¡Œè¡¨ç¤ºPOI$p_i$åˆ°å…¶ä»–æ¯ä¸ªPOIçš„æ¦‚ç‡ï¼ˆæœªå½’ä¸€åŒ–ï¼‰ã€‚å¯¹`GETNext`ä¸­æœ€åçš„`Transformerå±‚`ç”Ÿæˆçš„POIï¼Œå»$\Phi$ä¸­æŸ¥æ‰¾å¯¹åº”è¡Œçš„æ¦‚ç‡å€¼ï¼Œä»¥è°ƒæ•´æœ€ç»ˆçš„æ¨èç»“æœã€‚

#### Contextual Embedding Module
æ—¶ç©ºå› ç´ å’Œç”¨æˆ·åå¥½æ˜¯ä¸ªæ€§åŒ–`next POI recommendations`çš„å…³é”®å› ç´ ã€‚ä¸‹é¢å°†èåˆ`user embeddings`å’Œ`POI embeddings`ä¸º`POI-User Embeddings`ï¼Œèåˆ`POI category embeddings`å’Œ`time encoding`ä¸º`Time-Category Embeddings`ã€‚

##### POI-User Embeddings Fusion
å¯¹äº`user embeddings`ï¼Œè®­ç»ƒä¸€ä¸ª`embedding layer`å°†æ¯ä¸ªç”¨æˆ·æŠ•å½±åˆ°ä¸€ä¸ªä½ç»´å‘é‡ä¸Šï¼Œæ¯ä¸ªç”¨æˆ·çš„`embedding`æ˜¯ä»ç”¨æˆ·è‡ªå·±çš„å†å²`check-in`åºåˆ—ä¸­å­¦ä¹ çš„ã€‚å¯è¡¨ç¤ºä¸º**å…¬å¼(7)**$\mathrm{e}_u=f_{embed}(u)\in\mathbb{R}^\Omega$ã€‚
``` python
# def 'UserEmbeddings model' in model.py
# in train.py
# init model 
num_users = len(user_id2idx_dict)
user_embed_model = UserEmbeddings(num_users, args.user_embed_dim) # useråµŒå…¥æ¨¡å‹åˆå§‹åŒ–
# generate 'user embedding' in method input_traj_to_embeddings
user_id = traj_id.split('_')[0]
user_idx = user_id2idx_dict[user_id]
input = torch.LongTensor([user_idx]).to(device=args.device)
user_embedding = user_embed_model(input) # å…¬å¼(7)
user_embedding = torch.squeeze(user_embedding)
```

ä¸ºäº†æ„å»º`POI-User Embeddings`ï¼Œå…ˆå°†`POI embedding`å’Œ`user embedding`åš`concat`æ“ä½œï¼Œå†å°†å·²ä¸²è”çš„å‘é‡é€å…¥`dense layer`å»`fine-tune`å·²èåˆçš„åµŒå…¥ã€‚å¯è¡¨ç¤ºä¸ºå…¬å¼(8)$\mathrm{e}_{p,u}=\sigma(\mathrm{w}_{p,u}[\mathrm{e}_p;\mathrm{e}_u]+b_{p,u})\in\mathbb{R}^{\Omega\times 2}$ï¼Œå…¶ä¸­ï¼Œ$\mathrm{w}_{p,u}$ä¸ºæƒé‡å‘é‡ï¼Œ$b_{p,u}$ä¸ºåç½®ï¼Œ$[\cdot;\cdot]$è¡¨ç¤ºä¸²è”æ“ä½œã€‚èåˆåï¼ŒåµŒå…¥å‘é‡çš„å¤§å°ä¿æŒä¸å˜ï¼Œ`POI-User Embeddings`çš„ç»´åº¦æ˜¯`POI embedding`æˆ–`user embedding`ç»´åº¦çš„äºŒå€ã€‚
``` python
# in train.py
# generate fused POI-User Embedding in method input_traj_to_embeddings
fused_embedding1 = embed_fuse_model1(user_embedding, poi_embedding) # å…¬å¼(8)
```

##### Time-Category Embeddings Fusion
ç”¨æˆ·çš„è®¿é—®è¡Œä¸ºè‡ªç„¶æ˜¯æœ‰æ—¶é—´ä¾èµ–çš„ï¼Œå¦‚ä¸‹å›¾ï¼Œå±•ç¤ºäº†ä¸¤ä¸ªPOI Categoryï¼ˆâ€œtrain stationâ€å’Œâ€œbarâ€ï¼‰åœ¨ä¸åŒæ—¶é—´æ®µçš„è®¿é—®è¡Œä¸ºã€‚ä¾‹å¦‚ï¼Œå¯¹äº`next POI recommendation`ï¼Œæ—©ä¸Š8ç‚¹åº”è¯¥å»`train station`è€Œä¸æ˜¯å»`bar`ã€‚
![picture 17](assets/images/1685667502736.png)  

é¦–å…ˆï¼Œä½¿ç”¨`time2vector`å¯¹`time`ç¼–ç ã€‚å…·ä½“è¯´ï¼Œå°†24å°æ—¶åˆ’åˆ†ä¸º48ä¸ªæ—¶æ®µï¼Œæ¯ä¸ªæ—¶æ®µ30åˆ†é’Ÿï¼Œå°†ä¸€ä¸ªæ—¶é—´æ ‡é‡æŠ•å½±åˆ°æ—¶æ®µä¸Šã€‚æ—¶æ®µ$t$çš„åµŒå…¥å¯ä»¥è¡¨ç¤ºä¸º$\mathrm{e}_t$ï¼ˆæ˜¯ä¸€ä¸ªé•¿åº¦ä¸º$k+1$çš„å‘é‡ï¼‰ã€‚å‘é‡ä¸­ç¬¬$i$ä¸ªå…ƒç´ å¯ä»¥è¡¨ç¤ºä¸º**å…¬å¼(9)**$\mathrm{e}_t[i]=\begin{cases}
    \omega_i t+\varphi_i, & if\ i=0. \\
    \sin(\omega_i t+\varphi_i), & if\ 1 \leq i \leq k.
\end{cases}$ï¼Œå…¶ä¸­ï¼Œ$omega$å’Œ$\varphi$ä¸ºå¯è®­ç»ƒçš„å‚æ•°ï¼Œ$\sin$ä¸ºæ¿€æ´»å‡½æ•°ã€‚
``` python
# def 'time2vector model' in model.py
# in train.py
# init model
time_embed_model = Time2Vec('sin', out_dim=args.time_embed_dim) # timeåµŒå…¥æ¨¡å‹åˆå§‹åŒ–
# generate 'time embedding' in method input_traj_to_embeddings
time_embedding = time_embed_model(
    torch.tensor([input_seq_time[idx]], dtype=torch.float).to(device=args.device)) # å…¬å¼(9)
time_embedding = torch.squeeze(time_embedding).to(device=args.device)
```

ç„¶åï¼Œå¯¹`POI categories`ä½¿ç”¨`embedding layer`ï¼Œç±»åˆ«$c$çš„åµŒå…¥å¯ä»¥è¡¨ç¤ºä¸º**å…¬å¼(10)**$\mathrm{e}_c=f_{embed}(c)\in\mathbb{R}^\Psi$ï¼Œå…¶ä¸­$\Psi$ä¸º$\mathrm{e}_c$çš„ç»´åº¦ã€‚
``` python
# def 'CategoryEmbeddings model' in model.py
# in train.py
# init model
cat_embed_model = CategoryEmbeddings(num_cats, args.cat_embed_dim) # categoryåµŒå…¥æ¨¡å‹åˆå§‹åŒ–
# generate 'category embedding' in method input_traj_to_embeddings
cat_idx = torch.LongTensor([input_seq_cat[idx]]).to(device=args.device)
cat_embedding = cat_embed_model(cat_idx) # å…¬å¼(10)
cat_embedding = torch.squeeze(cat_embedding)
```

æœ€åï¼Œå¯¹`time embedding` $\mathrm{e}_t$å’Œ`category embedding` $\mathrm{e}_c$è¿›è¡Œèåˆï¼Œå¯ä»¥è¡¨ç¤ºä¸º**å…¬å¼(11)**$\mathrm{e}_{c,t}=\sigma(\mathrm{w}_{c,t}[\mathrm{e}_t;\mathrm{e}_c]+b_{c,t})\in\mathbb{R}^{\Psi\times 2}$ï¼Œå…¶ä¸­ï¼Œ$\mathrm{w}_{c,t}$ä¸ºå¯è®­ç»ƒçš„æƒé‡å‘é‡ï¼Œ$b_{c,t}$ä¸ºåç½®ã€‚
``` python
# def 'FuseEmbeddings model' in model.py
# in train.py
# init model
embed_fuse_model2 = FuseEmbeddings(args.time_embed_dim, args.cat_embed_dim) # time-category åµŒå…¥èåˆæ¨¡å‹åˆå§‹åŒ–
# generate fused Time-Category Embedding in method input_traj_to_embeddings
fused_embedding2 = embed_fuse_model2(time_embedding, cat_embedding) # å…¬å¼(11)
```

æœ€åçš„æœ€åï¼Œå°†ä¸¤ä¸ªå·²èåˆçš„åµŒå…¥ï¼ˆ$\mathrm{e}_{p,u}$å’Œ$\mathrm{e}_{c,t}$ï¼‰å†æ¬¡èåˆä¸º$\mathrm{e}_q=[\mathrm{e}_{p,u};\mathrm{e}_{c,t}]$ï¼Œ$\mathrm{e}_q$è¡¨ç¤º`check-in` $q=\lang p,u,t\rang$çš„åµŒå…¥ã€‚å› æ­¤ï¼Œæ¯ä¸ªè¾“å…¥è½¨è¿¹$(q_1,...,q_s)$å¯ä»¥ç”±`check-in embeddings`è¡¨ç¤ºä¸º$(\mathrm{e}_{q_1},...,\mathrm{e}_{q_s})$ã€‚å†å°†`check-in embeddings`ä¼ ç»™ä¸‹ä¸€å±‚`transformer encoder`ã€‚
``` python
# in train.py
# fuse 'POI-User Embeddings' and 'Time-Category Embeddings' in the method of input_traj_to_embeddings
concat_embedding = torch.cat((fused_embedding1, fused_embedding2), dim=-1) # åˆå¹¶4.3.1å’Œ4.3.2ä¸¤ä¸ªåµŒå…¥
```

#### Transformer Encoder and MLP Decoders
##### Transformer Encoder
ç›®æ ‡æ˜¯æ ¹æ®ç»™å‡ºçš„`check-in`åºåˆ—åªé¢„æµ‹`next immediate POI`ï¼Œæ‰€ä»¥åªé‡‡ç”¨`transformer encoder`å’Œå‡ ä¸ª`MLP`å±‚ï¼ˆæœªä½¿ç”¨`transformer decoder`ï¼‰ã€‚

ç»™å‡ºè¾“å…¥è½¨è¿¹$S_u=(q_u^1,q_u^2,...,q_u^k)$ï¼Œé¢„æµ‹åœ¨`next check-in activity`$q_u^{k+1}$ä¸­çš„POIã€‚